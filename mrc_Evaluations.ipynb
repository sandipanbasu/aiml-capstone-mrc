{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc_Evaluations.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUhLqpHo35mkb5nfLnEKQT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_Evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DRLVpTJznGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "33836c53-d630-4342-c7a9-a8e74a70d519"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOGCXAgWaIsd",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaepomVyaJfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "345237eb-b7e0-4a91-bd74-e6fc5b3e97ee"
      },
      "source": [
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Dropout,BatchNormalization,Flatten,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from numpy import array\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\""
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuIsS7WFh76y",
        "colab_type": "text"
      },
      "source": [
        "# Load Google Bucket as drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCaxxOEXh8GK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "385edc30-1619-4e48-82da-be1f10c1aa79"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'ai-ml-capstone'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "gs://aiml-capstone/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3NaG4vBiPvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a314c2a2-e466-4a0c-9a6b-fd563909c278"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   653  100   653    0     0  18138      0 --:--:-- --:--:-- --:--:-- 18138\n",
            "OK\n",
            "50 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 4,274 kB of archives.\n",
            "After this operation, 12.8 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.28.1_amd64.deb ...\n",
            "Unpacking gcsfuse (0.28.1) ...\n",
            "Setting up gcsfuse (0.28.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdNVzqJ_jfs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "58e6b169-3d42-4c2c-ea5e-3d8eb320285d"
      },
      "source": [
        "!mkdir gbucket\n",
        "!gcsfuse --implicit-dirs aiml-capstone gbucket \n",
        "# !umount gbucket"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using mount point: /content/gbucket\n",
            "Opening GCS connection...\n",
            "Opening bucket...\n",
            "Mounting file system...\n",
            "File system has been successfully mounted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in-L5Mi9kEir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58f97f72-6ef3-42a4-bafe-2104ab57d132"
      },
      "source": [
        "!ls gbucket/lstmbaseline-0/tf-serve/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved_model.pb\tvariables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUPKTSR-GWt5",
        "colab_type": "text"
      },
      "source": [
        "# List of Models\n",
        "\n",
        "As part of our capstone, we are in process of evaluating the following models\n",
        "\n",
        "Data | Model | On GPU | Masking | Padding | Epoch | Location | \n",
        "--- | --- | --- | --- | --- | --- | --- | \n",
        "Without stopwords | SVM | No | - | - | - | [here](https://storage.cloud.google.com/aiml-capstone/svm/)\n",
        "Without stopwords | LSTM Baseline | No | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5)\n",
        "Without stopwords | Deep LSTM + GloVe | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/deeplstm/full_context_withoutstopwords_model_epoch_deeplstm_gpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe | No | No | Pre | 10 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm/full_context_withoutstopwords_model_epoch_10_bilstm_cpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe + Q2C Attention | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm-q2c-attention-glove/full_context_withoutstopwords_nomask_epoch_25_bilstm_q2c-attention_glove_gpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe + Q2C-C2Q Attention | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu.h5)\n",
        "--- | --- | --- | --- | --- | --- | --- | \n",
        "With Stopwords | Bi-LSTM + Universal Sentence Encoder + Q2C Attention\n",
        "With Stopwords | Bi-LSTM + Universal Sentence Encoder + Q2C-C2Q Attention\n",
        "With Stopwords | BERT + Universal Sentence Encoder \n",
        "With Stopwords | GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_GOOYs6cXfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "39050a86-2e16-4c1c-8296-4980646f3a4b"
      },
      "source": [
        "# TODO \n",
        "#     \"bilstm-use-q2c-attention\":{\n",
        "#         \"name\":\"Bi-LSTM + Universal Sentence Encoder + Q2C Attention\",\n",
        "#         \"loc\":\"\"\n",
        "#     },\n",
        "#     \"bilstm-use-q2c-c2q-attention\":{\n",
        "#         \"name\":\"Bi-LSTM + Universal Sentence Encoder + Q2C-C2Q Attention\",\n",
        "#         \"loc\":\"\"\n",
        "#     },\n",
        "#     \"bert-use\":{\n",
        "#         \"name\":\"BERT + Universal Sentence Encoder\",\n",
        "#         \"loc\":\"\"\n",
        "#     },\n",
        "#     \"gpt\":{\n",
        "#         \"name\":\"GPT\",\n",
        "#         \"loc\":\"\"\n",
        "#     }    \n",
        "# }\n",
        "\n",
        "list_of_models = [\n",
        "                  {\n",
        "                    \"id\":\"lstm-baseline\",\n",
        "                    \"name\":\"LSTM Baseline\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/tf-serve/\"\n",
        "                  },\n",
        "                  {\n",
        "                    \"id\":\"deeplstm-glove\",\n",
        "                    \"name\":\"Deep LSTM + GloVe\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/tf-serve/\"\n",
        "                  },  \n",
        "                  {\n",
        "                    \"id\":\"bilstm-glove\",\n",
        "                    \"name\":\"Bi-LSTM + GloVe\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\"\n",
        "                  },\n",
        "                  {\n",
        "                    \"id\":\"bilstm-glove-q2c-attention\",\n",
        "                    \"name\":\"Bi-LSTM + GloVe + Q2C Attention\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/tf-serve/\"\n",
        "                  },  \n",
        "                  {\n",
        "                    \"id\":\"bilstm-bidaf-glove\",\n",
        "                    \"name\":\"Bi-LSTM + GloVe + Q2C-C2Q Attention\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/tf-serve/\"\n",
        "                  }                                    \n",
        "                  ]\n",
        "\n",
        "list_of_models\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'lstm-baseline',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/tf-serve/',\n",
              "  'name': 'LSTM Baseline'},\n",
              " {'id': 'deeplstm-glove',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/tf-serve/',\n",
              "  'name': 'Deep LSTM + GloVe'},\n",
              " {'id': 'bilstm-glove',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5',\n",
              "  'name': 'Bi-LSTM + GloVe'},\n",
              " {'id': 'bilstm-glove-q2c-attention',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/tf-serve/',\n",
              "  'name': 'Bi-LSTM + GloVe + Q2C Attention'},\n",
              " {'id': 'bilstm-bidaf-glove',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/tf-serve/',\n",
              "  'name': 'Bi-LSTM + GloVe + Q2C-C2Q Attention'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfm8h4IBSGbh",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEtw1J-aPecV",
        "colab_type": "text"
      },
      "source": [
        "## bAbi \n",
        "\n",
        "Reference Paper = https://arxiv.org/pdf/1502.05698.pdf \n",
        "\n",
        "GitHub - https://github.com/facebookarchive/bAbI-tasks\n",
        "\n",
        "![alt text](https://storage.cloud.google.com/aiml-capstone/Screenshot%202020-06-20%20at%2011.11.50%20AM.png)\n",
        "![alt text](https://storage.cloud.google.com/aiml-capstone/Screenshot%202020-06-20%20at%2011.11.03%20AM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ5nidQ8SVog",
        "colab_type": "text"
      },
      "source": [
        "## SQuAD test dataset and published dev set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACmlhKB1S1ZF",
        "colab_type": "text"
      },
      "source": [
        "Refer to this eval metrics - https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "\n",
        "Eval dataset from SQuAD - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
        "\n",
        "Test Dataset from training = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGFyqInm7ZD0",
        "colab_type": "text"
      },
      "source": [
        "## News Domain Specific Evaluations\n",
        "\n",
        "Test on Sample News Context, Question and Answer pairs ??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yw8C9nY7uXT",
        "colab_type": "text"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDyvisumUImJ",
        "colab_type": "text"
      },
      "source": [
        "# Common Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XtBcy2Qwm2-c"
      },
      "source": [
        "## Custom function for preprocessing of context and question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LFr3-S_Gm9FX",
        "colab": {}
      },
      "source": [
        "# remove unwanted chars\n",
        "# convert to lowercase\n",
        "# remove unwanted spaces\n",
        "# remove stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "## reference \n",
        "def decontracted(phrase):\n",
        "    \"\"\"\n",
        "    This function remooves punctuation from given sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    if(phrase is np.nan):\n",
        "      return 'impossible'      \n",
        "\n",
        "    try:      \n",
        "      # specific\n",
        "      phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "      phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "      # general\n",
        "      phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "      phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "      phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "      phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "      phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "      phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "      \n",
        "      # string operation\n",
        "      phrase = phrase.replace('\\\\r', ' ')\n",
        "      phrase = phrase.replace('\\\\\"', ' ')\n",
        "      phrase = phrase.replace('\\\\n', ' ')\n",
        "\n",
        "      phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase.lower())\n",
        "    except:\n",
        "      print(phrase)  \n",
        "    \n",
        "    return phrase\n",
        "\n",
        "def preprocess_text(corpus, text_lower_case=True, \n",
        "                      special_char_removal=True, stopword_removal=True, remove_digits=False):    \n",
        "    normalized_text = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # doc = decontracted(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits) \n",
        "\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc)\n",
        "\n",
        "        normalized_text.append(doc)\n",
        "        \n",
        "    return normalized_text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    #Using regex\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):  \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)                 \n",
        "    return ' '.join(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "20-D8HBeOf_p"
      },
      "source": [
        "## Answer Span from Context and Answer, and reverse for predicted spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vbM8z2AEjxKK",
        "colab": {}
      },
      "source": [
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    Returns tokenised words.\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def answer_span(context,ans):\n",
        "    \"\"\"\n",
        "    This funtion returns anwer span start index and end index.\n",
        "    \"\"\"\n",
        "    ans_token = tokenize(ans)\n",
        "    con_token = tokenize(context)\n",
        "    ans_len = len(ans_token)\n",
        "    \n",
        "    if ans_len!=0 and ans_token[0] in con_token:\n",
        "    \n",
        "        indices = [i for i, x in enumerate(con_token) if x == ans_token[0]]        \n",
        "        try:\n",
        "\n",
        "            if(len(indices)>1):\n",
        "                start = [i for i in indices if (con_token[i:i+ans_len] == ans_token) ]\n",
        "                end = start[0] + ans_len - 1\n",
        "                return start[0],end\n",
        "\n",
        "            else:\n",
        "                start = con_token.index(ans_token[0])\n",
        "                end = start + ans_len - 1\n",
        "                return start,end\n",
        "        except:\n",
        "            return -1,-1\n",
        "    else:\n",
        "        return -1,-1\n",
        "\n",
        "def span_to_answer(span, context):\n",
        "  con_token = tokenize(context)  \n",
        "  return ' '.join(con_token[span[0]:span[1]+1])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sveyxKK5j8q"
      },
      "source": [
        "## Update and persist params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K170rfN15qGP",
        "colab": {}
      },
      "source": [
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "\n",
        "def updateparams():\n",
        "  with open(model_path + \"params.json\", \"w\") as p: \n",
        "    p.write(json.dumps(params))\n",
        "  print(\"params.jsop updated and can be found in \", model_path + \"params.json\")  \n",
        "\n",
        "# updateparams()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yYMDmMJNU9mw",
        "colab": {}
      },
      "source": [
        "def showparams(params):\n",
        "  pprint.pprint(params)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SklWt_NbVzOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4f9346f3-1a36-448e-e3d4-c156c40962ae"
      },
      "source": [
        "def loadparams():\n",
        "  with open(model_path + \"params.json\") as f:\n",
        "    params = json.load(f)\n",
        "  return params  \n",
        "\n",
        "showparams(loadparams())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': [26062, 16],\n",
            " 'test_span_outofrange': 0,\n",
            " 'train_shape': [78183, 16],\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': [26061, 16],\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0TSjNkdXzuR"
      },
      "source": [
        "## Create a common function to generate sequences (useful in prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sq97Vq4kXTST",
        "colab": {}
      },
      "source": [
        "# function to generate sequences withg appropiate padding\n",
        "def generate_question_context_sequence(context,question,question_max_length,padding,context_max_length):\n",
        "  question_seq = tokenizer.texts_to_sequences(question)\n",
        "  context_seq = tokenizer.texts_to_sequences(context)\n",
        "  question_seq = preprocessing.sequence.pad_sequences(question_seq,\n",
        "                                                      maxlen=question_max_length,\n",
        "                                                      padding=padding)\n",
        "  context_seq = preprocessing.sequence.pad_sequences(context_seq,\n",
        "                                                     maxlen=context_max_length,\n",
        "                                                     padding=padding)\n",
        "  return context_seq, question_seq"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v78fhG3Tdhx5"
      },
      "source": [
        "## Create a common function to predict and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v6KDfM25dhZr",
        "colab": {}
      },
      "source": [
        "def predit_test(context, question):\n",
        "  # get sequence for context and question\n",
        "  c_ = preprocess_text(context)\n",
        "  q_ = preprocess_text(question,stopword_removal=False)\n",
        "  c,q = generate_question_context_sequence(c_, q_)  \n",
        "  y_ = model.predict([q,c])    \n",
        "  # # for i in range(26062):\n",
        "  s = np.argmax(y_[0,:params['context_max_length']])\n",
        "  e = np.argmax(y_[0,params['context_max_length']:])\n",
        "  answer = span_to_answer((s,e),c_[0])\n",
        "  \n",
        "  # print(c.shape,q.shape,y_.shape,s,e,answer)  \n",
        "  # print(s, e)\n",
        "  return c_,q_,[s,e],y_,answer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSJXR52VTWsf",
        "colab_type": "text"
      },
      "source": [
        "## Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FJHi14rHNTX",
        "colab": {}
      },
      "source": [
        "def load_tokenizer():\n",
        "  with open(model_path + \"tokenizer.pkl\",\"rb\") as infile:\n",
        "      tokenizer = pickle.load(infile)\n",
        "  print('Vocab Loaded - ',len(tokenizer.word_index))\n",
        "  return tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIjvBaQ4WFZQ",
        "colab_type": "text"
      },
      "source": [
        "## Load Test Data with Ground Truth Known"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYkWpLxxWFii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import resample,shuffle\n",
        "\n",
        "def load_data_withoutstopwords():\n",
        "  #### NOTE THE 2 data frames's\n",
        "  df_nostopwords = 'test_squad_data_final_context_withoutstopwords.csv'\n",
        "  # df_withstopwords = 'squad_data_final_withstopword_withpunctuation.csv'\n",
        "  test_squad_df = pd.read_csv(project_path+df_nostopwords)\n",
        "  test_squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  test_squad_df[\"answer_word_span\"] = test_squad_df[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "  print(test_squad_df.info())\n",
        "  return test_squad_df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH025QzmyMAJ",
        "colab_type": "text"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDX397bcyMZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_loss(y_true,logits):\n",
        "    \"\"\"\n",
        "    Custom loss function which minimises log_loss.\n",
        "    Referance https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n",
        "    \"\"\"\n",
        "    \n",
        "    #y_true = tf.cast(y_true,dtype=tf.int32)\n",
        "    #logits = tf.cast(logits,dtype=tf.float32)\n",
        "    \n",
        "    # breaking the tensor into two half's to get start and end label.\n",
        "    start_label = y_true[:,:params['context_max_length']]\n",
        "    end_label = y_true[:,params['context_max_length']:]\n",
        "    \n",
        "    # braking the logits tensor into start and end part for loss calcultion.\n",
        "    start_logit = logits[:,:params['context_max_length']]\n",
        "    end_logit = logits[:,params['context_max_length']:]\n",
        "    \n",
        "    start_loss = tf.keras.backend.categorical_crossentropy(start_label,start_logit)\n",
        "    end_loss = tf.keras.backend.categorical_crossentropy(end_label,end_logit)\n",
        "    \n",
        "#     start_loss = tf.losses.sparse_softmax_cross_entropy(labels=start_label, logits=start_logit)\n",
        "#     end_loss = tf.losses.sparse_softmax_cross_entropy(labels=end_label, logits=end_logit)\n",
        "    \n",
        "    # as per paer\n",
        "    \n",
        "    loss = start_loss + end_loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP43qWYkewja",
        "colab_type": "text"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv8x2nxpSTwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52d1cc36-7b6b-4f31-e6fd-560e08423bd6"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pSzIAlTe07P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "def load_mrc_model(model_path):\n",
        "  custom_objects = {\"logits_loss\": logits_loss}\n",
        "  new_model = keras.models.load_model(model_path,custom_objects=custom_objects)\n",
        "  ### Check its architecture\n",
        "  # new_model.summary()  \n",
        "  return new_model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2fUNm7uPWSm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1e7c608-d56f-4977-ef5d-3f4d92c91070"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "with strategy.scope():\n",
        "  model = load_mrc_model('/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5')\n",
        "  \n",
        "model.summary()\n",
        "# list_of_models['bilstm-glove-q2c-c2q-attention']['loc'].replace('https://storage.cloud.google.com/aiml-capstone/','gbucket/')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "QUESTION_INPUT (InputLayer)     [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_EMBEDDING (Embedding)  (None, 40, 300)      30255300    QUESTION_INPUT[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_INPUT (InputLayer)      [(None, 426)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_LSTM (Bidirectional)   (None, 256)          1140736     QUESTION_EMBEDDING[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_EMBEDDING (Embedding)   (None, 426, 300)     30255300    CONTEXT_INPUT[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BILINEAR_AS_SPAN (Dense)        (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BILINEAR_AE_SPAN (Dense)        (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_LSTM (Bidirectional)    (None, 426, 256)     1140736     CONTEXT_EMBEDDING[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AS_ADD_DIM (None, 256, 1)       0           BILINEAR_AS_SPAN[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AE_ADD_DIM (None, 256, 1)       0           BILINEAR_AE_SPAN[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_3 (Te (None, 426, 1)       0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_BILINEAR_AS_ADD_DIM_1\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_4 (Te (None, 426, 1)       0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_BILINEAR_AE_ADD_DIM_1\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AS_DEL_DIM (None, 426)          0           tf_op_layer_BatchMatMulV2_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AE_DEL_DIM (None, 426)          0           tf_op_layer_BatchMatMulV2_4[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AS_SOFTMAX (None, 426)          0           tf_op_layer_BILINEAR_AS_DEL_DIM_1\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AE_SOFTMAX (None, 426)          0           tf_op_layer_BILINEAR_AE_DEL_DIM_1\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AS_AE_CONC (None, 852)          0           tf_op_layer_BILINEAR_AS_SOFTMAX_1\n",
            "                                                                 tf_op_layer_BILINEAR_AE_SOFTMAX_1\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te (None, 426)          0           tf_op_layer_BILINEAR_AS_AE_CONCAT\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te (None, 426)          0           tf_op_layer_BILINEAR_AS_AE_CONCAT\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_START_PROBAB_1 (Ten (None, 426)          0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_END_PROBAB_1 (Tenso (None, 426)          0           tf_op_layer_strided_slice_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_PROBAB_2 (None, 426, 1)       0           tf_op_layer_START_PROBAB_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_PROBAB_3 (None, 1, 426)       0           tf_op_layer_END_PROBAB_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_5 (Te (None, 426, 426)     0           tf_op_layer_PREDICT_AS_PROBAB_2[0\n",
            "                                                                 tf_op_layer_PREDICT_AS_PROBAB_3[0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_AE_TOPTR (None, 426, 426)     0           tf_op_layer_BatchMatMulV2_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_MAX_1 (T (None, 426)          0           tf_op_layer_PREDICT_AS_AE_TOPTRIA\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AE_MAX_1 (T (None, 426)          0           tf_op_layer_PREDICT_AS_AE_TOPTRIA\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_AE_1 (Te (None, 852)          0           tf_op_layer_PREDICT_AS_MAX_1[0][0\n",
            "                                                                 tf_op_layer_PREDICT_AE_MAX_1[0][0\n",
            "==================================================================================================\n",
            "Total params: 62,923,656\n",
            "Trainable params: 2,413,056\n",
            "Non-trainable params: 60,510,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DURR8zGyUpy6",
        "colab_type": "text"
      },
      "source": [
        "## Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWgeXhRLUrj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_test_data():\n",
        "  test = pd.read_csv(model_path +'test-withoutstopwords.csv')\n",
        "  test.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "  test[\"answer_word_span\"] = test[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "  print(test.shape)\n",
        "  return test"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoXsJRF_Vah4",
        "colab_type": "text"
      },
      "source": [
        "## Create Answer Sequence for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rregOZyRS_Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_answer_sequence(test, context_length):\n",
        "  # for test data\n",
        "  y_test = []\n",
        "  for i in range(len(test)):    \n",
        "      s = np.zeros(context_length,dtype = \"int\")\n",
        "      e = np.zeros(context_length,dtype = \"int\")        \n",
        "      start,end = test[\"answer_word_span\"].iloc[i]    \n",
        "      s[start] = 1\n",
        "      e[end] = 1\n",
        "      y_test.append(np.concatenate((s,e)))\n",
        "  return y_test\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRy1AevTZZDc",
        "colab_type": "text"
      },
      "source": [
        "## Predict Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6fQjbMN2ZaAi",
        "colab": {}
      },
      "source": [
        "def predit_test(context, question):\n",
        "  # get sequence for context and question\n",
        "  c_ = preprocess_text(context)\n",
        "  q_ = preprocess_text(question,stopword_removal=False)\n",
        "  c,q = generate_question_context_sequence(c_, q_)  \n",
        "  y_ = model.predict([q,c])    \n",
        "  # # for i in range(26062):\n",
        "  s = np.argmax(y_[0,:params['context_max_length']])\n",
        "  e = np.argmax(y_[0,params['context_max_length']:])\n",
        "  answer = span_to_answer((s,e),c_[0])\n",
        "  \n",
        "  # print(c.shape,q.shape,y_.shape,s,e,answer)  \n",
        "  # print(s, e)\n",
        "  return c_,q_,[s,e],y_,answer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDeMi7IWeb6_",
        "colab_type": "text"
      },
      "source": [
        "## Create a combined y_test array having both start and end vectors in OHE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3S3SB2cfFR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_y(y, test_sample_size):\n",
        "  # argmax is used to get the index where the max value in a list appears, and hence \n",
        "  # for every index i, we can get the place of start and end token of the max probab\n",
        "  start = []\n",
        "  end = []\n",
        "  for i in range(test_sample_size):\n",
        "      start.append(np.argmax(y[i,:params['context_max_length']]))\n",
        "      end.append(np.argmax(y[i,params['context_max_length']:]))\n",
        "      \n",
        "  y_new = np.zeros((test_sample_size,params['context_max_length']))\n",
        "  for i in range(test_sample_size):\n",
        "      y_new[i,start[i]:end[i]+1] = 1\n",
        "  return y_new,start,end"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUd9-eqtioBM",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0e-TuininW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_metrics(y_true,y_pred):\n",
        "  acc_score = accuracy_score(y_true,y_pred)\n",
        "  macro_f1_score = f1_score(y_true,y_pred,average=\"macro\")\n",
        "  micro_f1_score = f1_score(y_true,y_pred,average=\"micro\")\n",
        "  return acc_score,macro_f1_score,micro_f1_score\n",
        "  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqs4OxyjszkY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8382bd51-4c38-4528-e285-95d3dafc74f3"
      },
      "source": [
        "accuracy_metrics([1,1,1],[0,1,1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, 0.4, 0.6666666666666666)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnoYgva4_iEI",
        "colab_type": "text"
      },
      "source": [
        "## Exact Match Count per Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf3h0HWD_iyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkEMMatch(test_data, start, end):\n",
        "  field_names = [\"True Answer\",\n",
        "                \"True AS and AE\",\n",
        "                \"Predict Answer\",\n",
        "                \"Predict AS and AE\"]\n",
        "  result_df = pd.DataFrame(columns=field_names)\n",
        "  print('Checking for equality match percentage')\n",
        "  for i in tqdm(range(test.shape[0])):  \n",
        "    values = [test['clean_answer'].iloc[i], \n",
        "              test['answer_word_span'].iloc[i],\n",
        "              span_to_answer([start_pred[i],end_pred[i]],test['clean_context'].iloc[i]),\n",
        "              (start_pred[i],end_pred[i])]\n",
        "    zipped = zip(field_names, values)\n",
        "    a_dictionary = dict(zipped)\n",
        "    result_df = result_df.append(a_dictionary,ignore_index=True)\n",
        "\n",
        "  ematch = result_df[result_df['Predict Answer'] == result_df['True Answer']].shape[0]  / test.shape[0]\n",
        "  del result_df\n",
        "\n",
        "  return ematch\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVPxnPRWWJ1V",
        "colab_type": "text"
      },
      "source": [
        "# Evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP13NC14X47t",
        "colab_type": "text"
      },
      "source": [
        "## Eval on test data\n",
        "\n",
        "Metrics would be F1 score micro, EM, Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAUtNL4tkGut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bbd5d538-6c96-4d4b-a200-752a4f7464c3"
      },
      "source": [
        "test = load_test_data()\n",
        "test.shape[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26062, 16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26062"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4xL4rRSV5JW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "1ae56694-0555-4ec1-afa9-aa09726ce2aa"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "model_results = pd.DataFrame(columns=['Model Name','Acc Score','Micro F1', 'Macro F1','EM'])\n",
        "with strategy.scope():\n",
        "  test = load_test_data()\n",
        "  params = loadparams()\n",
        "  showparams(params)\n",
        "  tokenizer = load_tokenizer()\n",
        "  y_test = create_answer_sequence(test,params['context_max_length'])\n",
        "  # y_test was a list changing to numpy array\n",
        "  y_test_fixed = np.array(y_test)\n",
        "  # compute for y_test though in this case it the max of 0 and 1 for \n",
        "  # the frist half od array size for start, and rest for end\n",
        "  y_test_new,_,_ = combine_y(y_test_fixed,test.shape[0])\n",
        "\n",
        "  test_context_sequence, test_question_sequence = generate_question_context_sequence(context=test[\"clean_context\"].values,\n",
        "                                    question=test[\"clean_question\"].values,\n",
        "                                    question_max_length=params['question_max_length'],\n",
        "                                    padding=params['question_pad_seq'],\n",
        "                                    context_max_length=params['context_max_length']\n",
        "                                    )\n",
        "\n",
        "\n",
        "  for model in list_of_models:\n",
        "    print('Loading model ',model['name'],' from ', model['loc'])  \n",
        "    tfmodel = load_mrc_model(model['loc'])\n",
        "    print(model['name'],' model successfuly. Starting eval')\n",
        "    # tfmodel.summary()  \n",
        "    y_prediction = tfmodel.predict([test_question_sequence,test_context_sequence])\n",
        "    y_prediction_new,start_pred,end_pred = combine_y(y_prediction,test.shape[0])\n",
        "    acc_score,macro_f1_score,micro_f1_score = accuracy_metrics(y_test_new,y_prediction_new)\n",
        "    emmatch = checkEMMatch(test,start_pred,end_pred)\n",
        "    values = [model['name'],\n",
        "              acc_score,\n",
        "              micro_f1_score,\n",
        "              macro_f1_score,\n",
        "              emmatch]\n",
        "    zipped = zip(model_results.columns, values)\n",
        "    a_dictionary = dict(zipped)\n",
        "    model_results = model_results.append(a_dictionary,ignore_index=True)              \n",
        "\n",
        "model_results.head(10)   \n",
        "model_results.to_csv(model_path + \"model_results_\" + datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\") + \".csv\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "(26062, 16)\n",
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': [26062, 16],\n",
            " 'test_span_outofrange': 0,\n",
            " 'train_shape': [78183, 16],\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': [26061, 16],\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n",
            "Vocab Loaded -  100850\n",
            "Loading model  LSTM Baseline  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/tf-serve/\n",
            "LSTM Baseline  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
            "100%|██████████| 26062/26062 [02:41<00:00, 161.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Deep LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/tf-serve/\n",
            "Deep LSTM + GloVe  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [02:39<00:00, 162.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Bi-LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\n",
            "Bi-LSTM + GloVe  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [02:39<00:00, 163.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Bi-LSTM + GloVe + Q2C Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/tf-serve/\n",
            "Bi-LSTM + GloVe + Q2C Attention  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [02:40<00:00, 162.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Bi-LSTM + GloVe + Q2C-C2Q Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/tf-serve/\n",
            "Bi-LSTM + GloVe + Q2C-C2Q Attention  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [02:38<00:00, 164.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>Acc Score</th>\n",
              "      <th>Micro F1</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>EM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM Baseline</td>\n",
              "      <td>0.356112</td>\n",
              "      <td>0.228822</td>\n",
              "      <td>0.003986</td>\n",
              "      <td>0.002686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Deep LSTM + GloVe</td>\n",
              "      <td>0.315594</td>\n",
              "      <td>0.221054</td>\n",
              "      <td>0.003977</td>\n",
              "      <td>0.002878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bi-LSTM + GloVe</td>\n",
              "      <td>0.361100</td>\n",
              "      <td>0.255439</td>\n",
              "      <td>0.003302</td>\n",
              "      <td>0.004259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bi-LSTM + GloVe + Q2C Attention</td>\n",
              "      <td>0.470647</td>\n",
              "      <td>0.337377</td>\n",
              "      <td>0.001502</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bi-LSTM + GloVe + Q2C-C2Q Attention</td>\n",
              "      <td>0.345944</td>\n",
              "      <td>0.260478</td>\n",
              "      <td>0.012515</td>\n",
              "      <td>0.011933</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Model Name  Acc Score  Micro F1  Macro F1        EM\n",
              "0                        LSTM Baseline   0.356112  0.228822  0.003986  0.002686\n",
              "1                    Deep LSTM + GloVe   0.315594  0.221054  0.003977  0.002878\n",
              "2                      Bi-LSTM + GloVe   0.361100  0.255439  0.003302  0.004259\n",
              "3      Bi-LSTM + GloVe + Q2C Attention   0.470647  0.337377  0.001502  0.000000\n",
              "4  Bi-LSTM + GloVe + Q2C-C2Q Attention   0.345944  0.260478  0.012515  0.011933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHx9KbHZ8pNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_results.to_csv(model_path + \"model_results_\" + datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\") + \".csv\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCkMUNLEFYwG",
        "colab_type": "text"
      },
      "source": [
        "## Out of domain handing \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bppIF0J_Fews",
        "colab_type": "text"
      },
      "source": [
        "### Different language \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmEXz0ORFhIw",
        "colab_type": "text"
      },
      "source": [
        "### Totally irrelevant question given a known context from train domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OOCtgDAFj-X",
        "colab_type": "text"
      },
      "source": [
        "### Totally different context say from Medicine and ask answer from it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwvNDiQRFZql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WulPny3Z00mm",
        "colab_type": "text"
      },
      "source": [
        "## Eval on manual input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R03OP9TxDc6P",
        "colab_type": "text"
      },
      "source": [
        "### TEST1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTNhlpswRbMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c='In the Mahayana, the Buddha tends not to be viewed as merely human, but as the earthly projection of a beginningless and endless, omnipresent being (see Dharmakaya) beyond the range and reach of thought. Moreover, in certain Mahayana sutras, the Buddha, Dharma and Sangha are viewed essentially as One: all three are seen as the eternal Buddha himself.'\n",
        "q='in what sutras are the buddha dharma and sangha viewed as one'\n",
        "\n",
        "# c_,q_,span,y_,answer = predit_test(test['context'].iloc[39],test['question'].iloc[39])\n",
        "c_,q_,span,y_,answer = predit_test([c],[q])\n",
        "print('ori c = ')\n",
        "pprint.pprint(test['context'].iloc[39])\n",
        "print('ori c c = ')\n",
        "pprint.pprint(test['clean_context'].iloc[39])\n",
        "print('ori q = ',test['clean_question'].iloc[39])\n",
        "print('new c')\n",
        "pprint.pprint(c_[0])\n",
        "print('new q',q_)\n",
        "\n",
        "print('predicted answer' ,answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkyTSbDp1Jfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}