{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc-LSTM-baseline0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_LSTM_baseline0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879mz4G6-lRH",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import Libraries, setting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8sHlEbfYnn",
        "colab_type": "code",
        "outputId": "2122e722-a3db-4d02-9ee9-3e94312490bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_po1L9T-V5d",
        "colab_type": "code",
        "outputId": "24d5e7fa-d512-4fdc-d032-3abb50ded13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTDFtsEtV7dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Dropout,BatchNormalization,Flatten,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from numpy import array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPD8_fgd8PhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will store the params as we go along in this object\n",
        "params = {}\n",
        "project_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/Squad2.0/TrainingDataset/\"\n",
        "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\"\n",
        "tensorboard_logpath  = \"/content/drive/My Drive/AIML-MRC-Capstone/models/tensorboard-logs/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09eGk3oWWCaP",
        "colab_type": "text"
      },
      "source": [
        "# Objective - LSTM Baseline 0 \n",
        "\n",
        "*   **Inputs: A question q = {q1, ..., qQ} of length Q and a context paragraph p = {p1, ..., pP } of length P.**\n",
        "*   **Output: An answer span {as, ae} where as is the index of the first answer token in p, ae is the index of the last answer token in p, 0 <= as, ae >= m, and ae >= as.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGZp4iLEHE8n",
        "colab_type": "text"
      },
      "source": [
        "## 2 Load Squad Data - Cleaned and curated (output of preprocessing step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFPpoxl7Afls",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_u1n8G3fge_",
        "colab_type": "code",
        "outputId": "e5689662-212d-4486-8b59-956155e414c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "squad_df = pd.read_csv(project_path+'squad_data_final.csv')\n",
        "squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "squad_df.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>when did beyonce start becoming popular</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>17</td>\n",
              "      <td>286</td>\n",
              "      <td>(269, 286)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>what areas did beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>19</td>\n",
              "      <td>226</td>\n",
              "      <td>(207, 226)</td>\n",
              "      <td>(21, 23)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     title  ... answer_word_span\n",
              "0  Beyoncé  ...         (-1, -1)\n",
              "1  Beyoncé  ...         (21, 23)\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U35P3ZvGAQQD",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Create Train, Validation and Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M9hW4xy-Suj",
        "colab_type": "code",
        "outputId": "7466922f-138a-426a-c0ef-a3d0582a74e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample,shuffle\n",
        "\n",
        "# train = resample(train)\n",
        "# train = shuffle(train,n_samples =50000)\n",
        "\n",
        "train,test = train_test_split(squad_df,test_size = 0.2)\n",
        "train,val = train_test_split(train,test_size=0.25)\n",
        "\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78183, 16)\n",
            "(26061, 16)\n",
            "(26062, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sAKjoZ7CIJ_",
        "colab_type": "code",
        "outputId": "8a63aff4-1084-4a19-e41d-20a7b40a495d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "train[\"answer_word_span\"] = train[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "test[\"answer_word_span\"] = test[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "val[\"answer_word_span\"] = val[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "train.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 78183 entries, 110000 to 28804\n",
            "Data columns (total 16 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   title                   78183 non-null  object \n",
            " 1   context                 78183 non-null  object \n",
            " 2   question                78183 non-null  object \n",
            " 3   id                      78183 non-null  object \n",
            " 4   answer_start            78183 non-null  int64  \n",
            " 5   answer                  52131 non-null  object \n",
            " 6   plausible_answer_start  26052 non-null  float64\n",
            " 7   plausible_answer        26052 non-null  object \n",
            " 8   is_impossible           78183 non-null  bool   \n",
            " 9   clean_context           78183 non-null  object \n",
            " 10  clean_question          78183 non-null  object \n",
            " 11  clean_answer            78183 non-null  object \n",
            " 12  answer_len              78183 non-null  int64  \n",
            " 13  answer_end              78183 non-null  int64  \n",
            " 14  answer_span             78183 non-null  object \n",
            " 15  answer_word_span        78183 non-null  object \n",
            "dtypes: bool(1), float64(1), int64(3), object(11)\n",
            "memory usage: 9.6+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6dcPCGiJrz1",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Build Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxoMIrC6EIgS",
        "colab_type": "code",
        "outputId": "df2d68c0-49b5-4b54-eb5d-cc6507dcff7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "params['tokenizer_num_words'] = 80000\n",
        "tokenizer = preprocessing.text.Tokenizer(num_words=params['tokenizer_num_words'])\n",
        "\n",
        "# NOTE: tokenizer is been made out of original dataset\n",
        "for text in tqdm([squad_df['clean_context'], squad_df['clean_question']]):  \n",
        "  tokenizer.fit_on_texts(text.values)\n",
        "\n",
        "# total tokenizer words\n",
        "params['vocab_size'] = len(tokenizer.word_index)\n",
        "\n",
        "### SAVE TOKENIZERS\n",
        "with open(model_path + \"tokenizer.pkl\",\"wb\") as f:\n",
        "    pickle.dump(tokenizer,f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:07<00:00,  3.64s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXSy_y36IFb",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Update parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whUHcmX5PwS",
        "colab_type": "code",
        "outputId": "85ba8a3b-8807-48e7-8a39-61426905588b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of context word length = 285\n",
        "# 99% percentile or question word lengt = 20\n",
        "context_length = 285\n",
        "question_length = 20\n",
        "params['train_shape'] = train.shape\n",
        "params['val_shape'] = val.shape\n",
        "params['test_shape'] = test.shape\n",
        "params['context_length_99'] = context_length # initialize with a high percentile\n",
        "params['question_length_99'] = question_length # initialize with a high percentile\n",
        "params['embedding_size'] = 100\n",
        "params['rnn_units'] = 256\n",
        "params['context_pad_seq'] = 'pre'\n",
        "params['question_pad_seq'] = 'pre'\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lRuVtp_7y51",
        "colab_type": "text"
      },
      "source": [
        "## 3 Vectorization / Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJqzOfW9ARO",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 Integer Sequence of Context and Question "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhuvjYmZ7m6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_clean_context_sequence = tokenizer.texts_to_sequences(train[\"clean_context\"].values)\n",
        "test_clean_context_sequence = tokenizer.texts_to_sequences(test[\"clean_context\"].values)\n",
        "val_clean_context_sequence = tokenizer.texts_to_sequences(val[\"clean_context\"].values)\n",
        "\n",
        "\n",
        "train_clean_question_sequence = tokenizer.texts_to_sequences(train[\"clean_question\"].values)\n",
        "test_clean_question_sequence = tokenizer.texts_to_sequences(test[\"clean_question\"].values)\n",
        "val_clean_question_sequence = tokenizer.texts_to_sequences(val[\"clean_question\"].values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u41PKZTFcwm",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 Find Max Sequence length of Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwh8HGs0Fbp2",
        "colab_type": "code",
        "outputId": "b5b5a615-889f-499a-e1f7-bf62b4ab9175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# max length of context\n",
        "params['context_max_length'] = max(max(len(txt) for txt in train_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_context_sequence))\n",
        "\n",
        "params['question_max_length'] = max(max(len(txt) for txt in train_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_question_sequence))\n",
        "\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhnBwThFIA0H",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3 Padding of the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGvIf7xU9rIJ",
        "colab_type": "code",
        "outputId": "6b2b921f-d77f-45db-ca41-ef12ce8c1994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_context_sequence = preprocessing.sequence.pad_sequences(train_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "\n",
        "print(\"Max context Sequence length is {}\".format(train_context_sequence.shape[1]))\n",
        "\n",
        "test_context_sequence = preprocessing.sequence.pad_sequences(test_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "val_context_sequence = preprocessing.sequence.pad_sequences(val_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "\n",
        "print(train_context_sequence.shape)\n",
        "print(test_context_sequence.shape)\n",
        "print(val_context_sequence.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max context Sequence length is 426\n",
            "(78183, 426)\n",
            "(26062, 426)\n",
            "(26061, 426)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NxpIGIb9p5T",
        "colab_type": "code",
        "outputId": "05d29ebb-d1bc-46c3-a1a5-be6f265a115e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_question_sequence = preprocessing.sequence.pad_sequences(train_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "print(\"Max Question Sequence length is {}\".format(train_question_sequence.shape[1]))\n",
        "test_question_sequence = preprocessing.sequence.pad_sequences(test_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "val_question_sequence = preprocessing.sequence.pad_sequences(val_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "\n",
        "print(train_question_sequence.shape)\n",
        "print(test_question_sequence.shape)\n",
        "print(val_question_sequence.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Question Sequence length is 40\n",
            "(78183, 40)\n",
            "(26062, 40)\n",
            "(26061, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgCKNbH6_-yd",
        "colab_type": "text"
      },
      "source": [
        "#### 3.4 Create Answer Sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv0yC_w8AF4x",
        "colab_type": "text"
      },
      "source": [
        "Encode y_trues as big array consisting of ans_start + ans_end. This has to be used in loss function as well. We will use the answer_word_span feature\n",
        "\n",
        "**y_true = answer_start + answer_end**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHPxRRHuAnSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for train data\n",
        "y_train = []\n",
        "span_ofr = 0;\n",
        "params['train_span_outofrange'] = 0\n",
        "params['test_span_outofrange'] = 0\n",
        "params['val_span_outofrange'] = 0\n",
        "\n",
        "for i in range(len(train)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    start, end = train[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    y_train.append(np.concatenate((s,e)))    \n",
        "\n",
        "params['train_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "\n",
        "# for test data\n",
        "y_test = []\n",
        "for i in range(len(test)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = test[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    y_test.append(np.concatenate((s,e)))\n",
        "\n",
        "params['test_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "                \n",
        "# for val data\n",
        "y_val = []\n",
        "for i in range(len(val)):\n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = val[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1      \n",
        "    y_val.append(np.concatenate((s,e)))\n",
        "\n",
        "params['val_span_outofrange'] = span_ofr    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgPgnMl0VZCd",
        "colab_type": "code",
        "outputId": "17f5e096-ada3-4ba9-f908-b8e7a23fcf0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(y_train),len(y_train[0]))\n",
        "print(len(y_test),len(y_test[0]))\n",
        "print(len(y_val),len(y_val[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78183 852\n",
            "26062 852\n",
            "26061 852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rZCyBydBZW1",
        "colab_type": "code",
        "outputId": "00c96fb0-f2bb-4de2-f20e-ccd5320d871d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "pprint.pprint(params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDzXjXa3MpP0",
        "colab_type": "code",
        "outputId": "309c4f8e-3eba-4dc5-852c-ffebd142cceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(squad_df['clean_context'][2000])\n",
        "print(context_sequence[3000])\n",
        "print(squad_df['clean_question'][2000])\n",
        "print(questions_sequence[2000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "october 21 2008 apple reported 14 21 total revenue fiscal quarter 4 year 2008 came ipods september 9 2009 keynote presentation apple event phil schiller announced total cumulative sales ipods exceeded 220 million continual decline ipod sales since 2009 surprising trend apple corporation apple cfo peter oppenheimer explained june 2009 expect traditional mp3 players decline time cannibalize ipod touch iphone since 2009 companys ipod sales continually decreased every financial quarter 2013 new model introduced onto market\n",
            "[384, 511, 58838, 520, 504, 34, 7655, 7507, 1434, 2740, 3028, 8, 22, 1434, 5566, 48288, 1070, 2828, 4793, 266, 1211, 9403, 2828, 5566, 7837, 3016, 1636, 4027, 697, 1922, 5566, 252, 2271, 4793, 27639, 99, 578, 5016, 142, 1434, 680, 5683, 142, 5566, 7, 58839, 1562, 11769, 4284, 17681, 255, 58840, 1050, 3760, 5209, 24179, 1084, 2740, 3028, 235, 1376, 115, 6740, 179, 1578, 374, 4793, 27639, 578, 8588, 44, 3439, 19, 455, 2820, 574, 193, 578, 87, 13, 11280, 60, 3366, 636]\n",
            "who was chief financial officer of apple in july of 2009\n",
            "[37, 11, 767, 520, 2272, 3, 859, 5, 335, 3, 281]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sveyxKK5j8q",
        "colab_type": "text"
      },
      "source": [
        "### 3.5 Update and persist params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K170rfN15qGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "with open(model_path + \"params.json\", \"w\") as p: \n",
        "    p.write(json.dumps(params))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FSl5UDL_qD",
        "colab_type": "text"
      },
      "source": [
        "## 4 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS6fKEvieWrX",
        "colab_type": "text"
      },
      "source": [
        "**Implements a baseline 0 in Deep Learning based approach per our project synopsis. This baseline model uses the following layers **\n",
        "0.   Input layer\n",
        "1.   Embedding Layer\n",
        "2.   List LSTM\n",
        "3.   a custom Bilinear Similarity layer \n",
        "4.   Prediction Layer\n",
        "5.   Output layer \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9JFn3oWiU4y",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Building Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLGurD9eijTh",
        "colab_type": "text"
      },
      "source": [
        "**For Questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLqUQHSjeVwq",
        "colab_type": "code",
        "outputId": "5aa17dbb-4dd2-48de-f750-48773079aae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# question embedding\n",
        "q_input = layers.Input(shape=(params['question_max_length'],),name=\"QUESTION_INPUT\")\n",
        "q_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
        "                  output_dim=params['embedding_size'],\n",
        "                  name=\"QUESTION_EMBEDDING\")(q_input)\n",
        "\n",
        "# encoder \n",
        "q_output = layers.LSTM(units=params['rnn_units'], \n",
        "                     name='QUESTION_LSTM')(q_emb)\n",
        "print(q_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eJ2ykC1megw",
        "colab_type": "text"
      },
      "source": [
        "**For Context**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glhG509P5Yh",
        "colab_type": "code",
        "outputId": "00ef102d-c7d7-48d7-d923-73af6b2e2a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c_input = layers.Input(shape=(params['context_max_length'],),name=\"CONTEXT_INPUT\")\n",
        "\n",
        "# context embedding\n",
        "c_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
        "                  output_dim=params['embedding_size'],\n",
        "                  name=\"CONTEXT_EMBEDDING\")(c_input)\n",
        "\n",
        "\n",
        "\n",
        "# exact_match\n",
        "# ex_ = Input(shape=(CON_LEN,2))\n",
        "\n",
        "# # pos tags\n",
        "# pos_ = Input(shape=(CON_LEN,len(tag_to_num)+1))\n",
        "\n",
        "# # term frequency\n",
        "# term_ = Input(shape=(CON_LEN,1)) \n",
        "\n",
        "# concatenate input\n",
        "# concat = concatenate([c_emb,ex_,pos_,term_])\n",
        "\n",
        "c_output = layers.LSTM(params['rnn_units'],\n",
        "                 name='CONTEXT_LSTM',return_sequences=True)(c_emb)\n",
        "\n",
        "print(\"final output to bilinear \",c_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final output to bilinear  (None, 426, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g93054zrp9yp",
        "colab_type": "text"
      },
      "source": [
        "**Bilinear Term**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsrWO_tpppa",
        "colab_type": "code",
        "outputId": "69a86ced-e6e8-4f27-b08e-d65d1a7a239d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Reference -- https://github.com/kellywzhang/reading-comprehension/blob/master/attention.py\n",
        "# bilinear term ####\n",
        "print(\"Question context shape \",q_output.shape)\n",
        "print(\"final o/p of context \",c_output.shape)\n",
        "\n",
        "################ start prediction ######################\n",
        "start = layers.Dense(params['rnn_units'])(q_output)\n",
        "\n",
        "# ading time_slice to question (batch_size,1,hidden)\n",
        "# shape (64,128) --> (64,1,128)\n",
        "hidden_start_time_axis = tf.expand_dims(start, -1)\n",
        "\n",
        "# squeeze remooves time slice we added before\n",
        "# final shape = (batch_size,decoder_timesteps)\n",
        "start_ = tf.squeeze(tf.matmul(c_output,hidden_start_time_axis),-1)\n",
        "    \n",
        "start_ = tf.nn.softmax(start_,axis = 1)\n",
        "    \n",
        "################ end prediction ######################\n",
        "end = layers.Dense(params['rnn_units'])(q_output)\n",
        "\n",
        "hidden_end_time_axis = tf.expand_dims(end, -1)\n",
        "\n",
        "# squeeze remooves time slice we added before\n",
        "# final shape = (batch_size,decoder_timesteps)\n",
        "end_ = tf.squeeze(tf.matmul(c_output,hidden_end_time_axis),-1)\n",
        "end_ = tf.nn.softmax(end_,axis=1)\n",
        "\n",
        "prob_token_span = tf.concat((start_,end_),axis = 1)\n",
        "print(\"Logits shape \",prob_token_span.shape)\n",
        "\n",
        "\n",
        "# logits = BilinearSimilarity(UNITS)(q_cont,c_)\n",
        "# Y_prob = Prediction()(logits)\n",
        "# print(\"Logits shape \",logits.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question context shape  (None, 256)\n",
            "final o/p of context  (None, 426, 256)\n",
            "Logits shape  (None, 852)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibcGeRqUxzKN",
        "colab_type": "text"
      },
      "source": [
        "**Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFbuhviAyX7l",
        "colab_type": "code",
        "outputId": "a6e3cb26-3fbe-452f-dfa4-e66f60b3edc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "####### Prediction ### \n",
        "token_span = 20\n",
        "start_prob = prob_token_span[:,:params['context_max_length']]\n",
        "end_prob = prob_token_span[:,params['context_max_length']:]\n",
        "\n",
        "# do the outer product\n",
        "outer = tf.matmul(tf.expand_dims(start_prob, axis=2),tf.expand_dims(end_prob, axis=1))\n",
        "\n",
        "outer = tf.linalg.band_part(outer, 0, token_span)\n",
        "\n",
        "#print(outer.shape)\n",
        "\n",
        "# start_position will have shape of (batch_size,)\n",
        "start_position = tf.reduce_max(outer, axis=2)\n",
        "#end position will have shape of (batch_size,)\n",
        "end_position = tf.reduce_max(outer, axis=1)\n",
        "\n",
        "y_probab = tf.concat([start_position,end_position],axis=1)\n",
        "\n",
        "print(y_probab.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 852)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLtlPSfLxyzB",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 Custom Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlOkeMveyCWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_loss(y_true,logits):\n",
        "    \"\"\"\n",
        "    Custom loss function which minimises log_loss.\n",
        "    Referance https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n",
        "    \"\"\"\n",
        "    \n",
        "    #y_true = tf.cast(y_true,dtype=tf.int32)\n",
        "    #logits = tf.cast(logits,dtype=tf.float32)\n",
        "    \n",
        "    # breaking the tensor into two half's to get start and end label.\n",
        "    start_label = y_true[:,:params['context_max_length']]\n",
        "    end_label = y_true[:,params['context_max_length']:]\n",
        "    \n",
        "    # braking the logits tensor into start and end part for loss calcultion.\n",
        "    start_logit = logits[:,:params['context_max_length']]\n",
        "    end_logit = logits[:,params['context_max_length']:]\n",
        "    \n",
        "    start_loss = tf.keras.backend.categorical_crossentropy(start_label,start_logit)\n",
        "    end_loss = tf.keras.backend.categorical_crossentropy(end_label,end_logit)\n",
        "    \n",
        "#     start_loss = tf.losses.sparse_softmax_cross_entropy(labels=start_label, logits=start_logit)\n",
        "#     end_loss = tf.losses.sparse_softmax_cross_entropy(labels=end_label, logits=end_logit)\n",
        "    \n",
        "    # as per paer\n",
        "    \n",
        "    loss = start_loss + end_loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNiC79-tyLmq",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv9aw2EewJbr",
        "colab_type": "code",
        "outputId": "0d865f6e-4e84-4a33-ccd2-74b0bafa941c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs = [q_input,c_input],outputs =y_probab)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "QUESTION_INPUT (InputLayer)     [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_EMBEDDING (Embedding)  (None, 40, 100)      10085100    QUESTION_INPUT[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_INPUT (InputLayer)      [(None, 426)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_LSTM (LSTM)            (None, 256)          365568      QUESTION_EMBEDDING[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_EMBEDDING (Embedding)   (None, 426, 100)     10085100    CONTEXT_INPUT[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_LSTM (LSTM)             (None, 426, 256)     365568      CONTEXT_EMBEDDING[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_6 (Tenso [(None, 256, 1)]     0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_7 (Tenso [(None, 256, 1)]     0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_5 (Te [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_ExpandDims_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_6 (Te [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_ExpandDims_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_4 (TensorFl [(None, 426)]        0           tf_op_layer_BatchMatMulV2_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_5 (TensorFl [(None, 426)]        0           tf_op_layer_BatchMatMulV2_6[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softmax_4 (TensorFl [(None, 426)]        0           tf_op_layer_Squeeze_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softmax_5 (TensorFl [(None, 426)]        0           tf_op_layer_Squeeze_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3 (TensorFlo [(None, 852)]        0           tf_op_layer_Softmax_4[0][0]      \n",
            "                                                                 tf_op_layer_Softmax_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [(None, 426)]        0           tf_op_layer_concat_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te [(None, 426)]        0           tf_op_layer_concat_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_8 (Tenso [(None, 426, 1)]     0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_9 (Tenso [(None, 1, 426)]     0           tf_op_layer_strided_slice_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_7 (Te [(None, 426, 426)]   0           tf_op_layer_ExpandDims_8[0][0]   \n",
            "                                                                 tf_op_layer_ExpandDims_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MatrixBandPart_1 (T [(None, 426, 426)]   0           tf_op_layer_BatchMatMulV2_7[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Max_2 (TensorFlowOp [(None, 426)]        0           tf_op_layer_MatrixBandPart_1[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Max_3 (TensorFlowOp [(None, 426)]        0           tf_op_layer_MatrixBandPart_1[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_4 (TensorFlo [(None, 852)]        0           tf_op_layer_Max_2[0][0]          \n",
            "                                                                 tf_op_layer_Max_3[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 21,032,920\n",
            "Trainable params: 21,032,920\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Biepd_j518BQ",
        "colab_type": "text"
      },
      "source": [
        "### 4.5 Model Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAqT6LWbXxWE",
        "colab_type": "text"
      },
      "source": [
        "**Tensorboard Logs and Model compilation** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTB5bE8I2Al-",
        "colab_type": "code",
        "outputId": "e59c5503-872d-4c69-fd74-8e8a214c8117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# using tensorboard instance for callbacks\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "from tensorflow.python.keras.callbacks import TensorBoard\n",
        "\n",
        "log_dir = tensorboard_logpath +\"lstm-baseline0\"\n",
        "print('Tensprflow logs ',log_dir)\n",
        "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
        "\n",
        "# model compilation\n",
        "model.compile(optimizer=\"adamax\",loss=logits_loss,metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensprflow logs  /content/drive/My Drive/AIML-MRC-Capstone/models/tensorboard-logs/lstm-baseline0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WuaPHlU8hnp",
        "colab_type": "text"
      },
      "source": [
        "### 4.6 Generator Function for use in Model.fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYIgojpb8g82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Reference \n",
        "def generator_function(length,batch_size = 64,data_type = 'Train'):\n",
        "    \"\"\"\n",
        "    This function is generates batches of data to avoid strain on memory.\n",
        "    \"\"\"\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    flag = True\n",
        "    if data_type == 'Val':\n",
        "        flag = False\n",
        "    n = 0\n",
        "    # loop forever over datapoints.\n",
        "    while 1:\n",
        "        for i in range(length):\n",
        "            n += 1\n",
        "            if flag:\n",
        "                X1.append(train_question_sequence[i])\n",
        "                X2.append(train_context_sequence[i])                \n",
        "                y.append(y_train[i])\n",
        "            else:\n",
        "                X1.append(val_question_sequence[i])\n",
        "                X2.append(val_context_sequence[i])                \n",
        "                y.append(y_val[i])\n",
        "            if n == batch_size:\n",
        "                yield ((array(X1),array(X2)),array(y))\n",
        "                X1,X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlGf9CAe-ZW-",
        "colab_type": "text"
      },
      "source": [
        "### 4.7 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO-l9AhD-fPM",
        "colab_type": "code",
        "outputId": "d4225b5d-005a-4223-fb9c-72b12fb5727e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "params['training.epochs']=25\n",
        "params['training.batch_size']=64\n",
        "params['training.train_length']=len(y_train)\n",
        "params['training.val_length']=len(y_val)\n",
        "params['training.train_steps']=params['training.train_length']//params['training.batch_size']\n",
        "params['training.val_steps']=params['training.val_length']//32\n",
        "\n",
        "pprint.pprint(params)\n",
        "\n",
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "with open(model_path + \"params.json\", \"w\") as p: \n",
        "    p.write(json.dumps(params))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfbn-RnO_EOc",
        "colab_type": "code",
        "outputId": "fef1bf16-45a1-4e23-d188-f63c28d155f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for i in range(params['training.epochs']):\n",
        "    print(\"Epoch {} start at time \".format(i),datetime.now())\n",
        "    \n",
        "    train_generator = generator_function(params['training.train_length'],\n",
        "                                         params['training.batch_size'])\n",
        "    \n",
        "    val_generator = generator_function(params['training.val_length'],\n",
        "                                       32,\n",
        "                                       \"Val\")\n",
        "    model.fit(x=train_generator, epochs=1, \n",
        "                        steps_per_epoch=params['training.train_steps'],\n",
        "                        verbose=1,\n",
        "                        callbacks=[tensorboard],\n",
        "                        validation_data=val_generator,\n",
        "                        validation_steps=params['training.val_steps'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 start at time  2020-05-31 02:06:29.202658\n",
            "1221/1221 [==============================] - 136s 111ms/step - loss: 8.3468 - accuracy: 0.2365 - val_loss: 8.1262 - val_accuracy: 0.2791\n",
            "Epoch 1 start at time  2020-05-31 02:08:48.412338\n",
            "1113/1221 [==========================>...] - ETA: 10s - loss: 7.3605 - accuracy: 0.3453"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRoOUajk492o",
        "colab_type": "text"
      },
      "source": [
        "### 4.6 Serialize and Persist Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnnjf-hX2FD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(model_path + \"model_epoch_{}.h5\".format(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tVohk7bORpH",
        "colab_type": "text"
      },
      "source": [
        "### 4.7 Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7pxr5yHOV6m",
        "colab_type": "text"
      },
      "source": [
        "**Eval on Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VEUBrVHN6c_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_prediction = model.predict([test_question_sequence,test_context_sequence])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnVyS4ONOiNO",
        "colab_type": "code",
        "outputId": "a9b8ea33-1061-42e7-e4f5-194e6a50ab20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_prediction.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26062"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs9NJGGPP60m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_fixed = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xTLoptoRwZ0",
        "colab_type": "code",
        "outputId": "4da49e73-851b-4217-d99b-1af641694db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "y_test_fixed[0,:params['context_max_length']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_XCfB-gOpuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_pred = []\n",
        "end_pred = []\n",
        "for i in range(26062):\n",
        "    start_pred.append(np.argmax(y_prediction[i,:params['context_max_length']]))\n",
        "    end_pred.append(np.argmax(y_prediction[i,params['context_max_length']:]))\n",
        "    \n",
        "start = []\n",
        "end = []\n",
        "for i in range(26062):\n",
        "    start.append(np.argmax(y_test_fixed[i,:params['context_max_length']]))\n",
        "    end.append(np.argmax(y_test_fixed[i,params['context_max_length']:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwqdJBsmR2rt",
        "colab_type": "code",
        "outputId": "0209f673-9f34-4eef-ca8f-42aac8a70bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(start_pred[100:120])\n",
        "print(end_pred[100:120])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[425, 10, 425, 425, 425, 425, 423, 425, 49, 425, 425, 425, 425, 425, 425, 0, 29, 425, 9, 425]\n",
            "[425, 10, 425, 425, 425, 425, 423, 425, 49, 425, 425, 425, 425, 425, 425, 0, 29, 425, 9, 425]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "138CmIXnR7LS",
        "colab_type": "code",
        "outputId": "cd8241f5-1a45-4e06-e6b4-109a9b14c1bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(start[100:120])\n",
        "print(end[100:120])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[425, 425, 425, 20, 36, 3, 84, 425, 425, 425, 425, 70, 425, 19, 24, 8, 25, 74, 1, 49]\n",
            "[425, 425, 425, 22, 40, 3, 85, 425, 425, 425, 425, 70, 425, 20, 26, 9, 28, 75, 1, 54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPx7_OKMR8fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predicted_new = np.zeros((26062,params['context_max_length']))\n",
        "for i in range(26062):\n",
        "    y_predicted_new[i,start_pred[i]:end_pred[i]+1] = 1\n",
        "    \n",
        "y_test_new = np.zeros((26062,params['context_max_length']))\n",
        "for i in range(26062):\n",
        "    y_test_new[i,start[i]:end[i]+1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmwzLM3WSriL",
        "colab_type": "code",
        "outputId": "1293cbe3-a53d-455d-ce3c-f1556d3ac554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score\n",
        "print(\"Micro f1-score on test data is \",f1_score(y_test_new,y_predicted_new,average=\"micro\"))\n",
        "print(\"Macro f1-score on test data is \",f1_score(y_test_new,y_predicted_new,average=\"macro\"))\n",
        "print(\"Accuracy on test data is \",accuracy_score(y_test_new,y_predicted_new))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Micro f1-score on test data is  0.24381819178683042\n",
            "Macro f1-score on test data is  0.0029310912484228793\n",
            "Accuracy on test data is  0.3332054331977592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5T9BSNgS-uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}