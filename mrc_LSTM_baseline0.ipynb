{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc-LSTM-baseline0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_LSTM_baseline0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879mz4G6-lRH",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import Libraries, setting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8sHlEbfYnn",
        "colab_type": "code",
        "outputId": "ef10beda-0abb-4159-bc4f-ca14d9fcf708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_po1L9T-V5d",
        "colab_type": "code",
        "outputId": "996f3af4-9d55-44a1-a558-cb164bbd9784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTDFtsEtV7dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Dropout,BatchNormalization,Flatten,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPD8_fgd8PhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will store the params as we go along in this object\n",
        "params = {}\n",
        "project_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/Squad2.0/TrainingDataset/\"\n",
        "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09eGk3oWWCaP",
        "colab_type": "text"
      },
      "source": [
        "# Objective - LSTM Baseline 0 \n",
        "\n",
        "*   **Inputs: A question q = {q1, ..., qQ} of length Q and a context paragraph p = {p1, ..., pP } of length P.**\n",
        "*   **Output: An answer span {as, ae} where as is the index of the first answer token in p, ae is the index of the last answer token in p, 0 <= as, ae >= m, and as >= ae.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGZp4iLEHE8n",
        "colab_type": "text"
      },
      "source": [
        "## 2 Load Squad Data - Cleaned and curated (output of preprocessing step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFPpoxl7Afls",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_u1n8G3fge_",
        "colab_type": "code",
        "outputId": "1feb2134-0962-4df7-82b5-ccba2a935af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "squad_df = pd.read_csv(project_path+'squad_data_final.csv')\n",
        "squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "squad_df.head(2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>when did beyonce start becoming popular</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>17</td>\n",
              "      <td>286</td>\n",
              "      <td>(269, 286)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>what areas did beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>19</td>\n",
              "      <td>226</td>\n",
              "      <td>(207, 226)</td>\n",
              "      <td>(21, 23)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     title  ... answer_word_span\n",
              "0  Beyoncé  ...         (-1, -1)\n",
              "1  Beyoncé  ...         (21, 23)\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U35P3ZvGAQQD",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Create Train, Validation and Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M9hW4xy-Suj",
        "colab_type": "code",
        "outputId": "09f826a1-08de-4484-f4ad-dd3c1cc2a7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample,shuffle\n",
        "\n",
        "# train = resample(train)\n",
        "# train = shuffle(train,n_samples =50000)\n",
        "\n",
        "train,test = train_test_split(squad_df,test_size = 0.2)\n",
        "train,val = train_test_split(train,test_size=0.25)\n",
        "\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78183, 16)\n",
            "(26061, 16)\n",
            "(26062, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sAKjoZ7CIJ_",
        "colab_type": "code",
        "outputId": "3e8949cb-391c-44f2-a814-aa67a63e79b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train[\"answer_word_span\"] = train[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "test[\"answer_word_span\"] = test[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "val[\"answer_word_span\"] = val[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "train.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 78183 entries, 16265 to 53443\n",
            "Data columns (total 16 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   title                   78183 non-null  object \n",
            " 1   context                 78183 non-null  object \n",
            " 2   question                78183 non-null  object \n",
            " 3   id                      78183 non-null  object \n",
            " 4   answer_start            78183 non-null  int64  \n",
            " 5   answer                  51906 non-null  object \n",
            " 6   plausible_answer_start  26276 non-null  float64\n",
            " 7   plausible_answer        26276 non-null  object \n",
            " 8   is_impossible           78183 non-null  bool   \n",
            " 9   clean_context           78183 non-null  object \n",
            " 10  clean_question          78183 non-null  object \n",
            " 11  clean_answer            78183 non-null  object \n",
            " 12  answer_len              78183 non-null  int64  \n",
            " 13  answer_end              78183 non-null  int64  \n",
            " 14  answer_span             78183 non-null  object \n",
            " 15  answer_word_span        78183 non-null  object \n",
            "dtypes: bool(1), float64(1), int64(3), object(11)\n",
            "memory usage: 9.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6dcPCGiJrz1",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Build Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxoMIrC6EIgS",
        "colab_type": "code",
        "outputId": "6071b670-085c-46b8-d0d3-78b5c5393360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "params['tokenizer_num_words'] = 80000\n",
        "tokenizer = preprocessing.text.Tokenizer(num_words=params['tokenizer_num_words'])\n",
        "\n",
        "# NOTE: tokenizer is been made out of original dataset\n",
        "for text in tqdm([squad_df['clean_context'], squad_df['clean_question']]):  \n",
        "  tokenizer.fit_on_texts(text.values)\n",
        "\n",
        "# total tokenizer words\n",
        "params['vocab_size'] = len(tokenizer.word_index)\n",
        "\n",
        "### SAVE TOKENIZERS\n",
        "with open(model_path + \"tokenizer.pkl\",\"wb\") as f:\n",
        "    pickle.dump(tokenizer,f)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:07<00:00,  3.82s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXSy_y36IFb",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Update parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whUHcmX5PwS",
        "colab_type": "code",
        "outputId": "a5674208-4c94-4c8c-9a1b-6b951f8fb0da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of context word length = 285\n",
        "# 99% percentile or question word lengt = 20\n",
        "context_length = 285\n",
        "question_length = 20\n",
        "params['train_shape'] = train.shape\n",
        "params['val_shape'] = val.shape\n",
        "params['test_shape'] = test.shape\n",
        "params['context_length_99'] = context_length # initialize with a high percentile\n",
        "params['question_length_99'] = question_length # initialize with a high percentile\n",
        "params['embedding_size'] = 100\n",
        "params['rnn_units'] = 256\n",
        "params['context_pad_seq'] = 'pre'\n",
        "params['question_pad_seq'] = 'pre'\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lRuVtp_7y51",
        "colab_type": "text"
      },
      "source": [
        "## 3 Vectorization / Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJqzOfW9ARO",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 Integer Sequence of Context and Question "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhuvjYmZ7m6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_clean_context_sequence = tokenizer.texts_to_sequences(train[\"clean_context\"].values)\n",
        "test_clean_context_sequence = tokenizer.texts_to_sequences(test[\"clean_context\"].values)\n",
        "val_clean_context_sequence = tokenizer.texts_to_sequences(val[\"clean_context\"].values)\n",
        "\n",
        "\n",
        "train_clean_question_sequence = tokenizer.texts_to_sequences(train[\"clean_question\"].values)\n",
        "test_clean_question_sequence = tokenizer.texts_to_sequences(test[\"clean_question\"].values)\n",
        "val_clean_question_sequence = tokenizer.texts_to_sequences(val[\"clean_question\"].values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjiPxU_wFbuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u41PKZTFcwm",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 Find Max Sequence length of Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwh8HGs0Fbp2",
        "colab_type": "code",
        "outputId": "f58ea4fc-d763-45bd-b0e8-0f691a23fc64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# max length of context\n",
        "params['context_max_length'] = max(max(len(txt) for txt in train_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_context_sequence))\n",
        "\n",
        "params['question_max_length'] = max(max(len(txt) for txt in train_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_question_sequence))\n",
        "\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhnBwThFIA0H",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3 Padding of the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGvIf7xU9rIJ",
        "colab_type": "code",
        "outputId": "beaffeab-461f-43ce-c31f-0ecb8682b3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_context_sequence = preprocessing.sequence.pad_sequences(train_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "\n",
        "print(\"Max context Sequence length is {}\".format(train_context_sequence.shape[1]))\n",
        "\n",
        "test_context_sequence = preprocessing.sequence.pad_sequences(test_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "val_context_sequence = preprocessing.sequence.pad_sequences(val_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "\n",
        "print(train_context_sequence.shape)\n",
        "print(test_context_sequence.shape)\n",
        "print(val_context_sequence.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max context Sequence length is 426\n",
            "(78183, 426)\n",
            "(26062, 426)\n",
            "(26061, 426)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NxpIGIb9p5T",
        "colab_type": "code",
        "outputId": "93323d01-bd06-41be-b6a7-77e9cfae5089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_question_sequence = preprocessing.sequence.pad_sequences(train_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "print(\"Max Question Sequence length is {}\".format(train_question_sequence.shape[1]))\n",
        "test_question_sequence = preprocessing.sequence.pad_sequences(test_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "val_question_sequence = preprocessing.sequence.pad_sequences(val_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "\n",
        "print(train_question_sequence.shape)\n",
        "print(test_question_sequence.shape)\n",
        "print(val_question_sequence.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Question Sequence length is 40\n",
            "(78183, 40)\n",
            "(26062, 40)\n",
            "(26061, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgCKNbH6_-yd",
        "colab_type": "text"
      },
      "source": [
        "#### 3.4 Create Answer Sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv0yC_w8AF4x",
        "colab_type": "text"
      },
      "source": [
        "Encode y_trues as big array consisting of ans_start + ans_end. This has to be used in loss function as well. We will use the answer_word_span feature\n",
        "\n",
        "**y_true = answer_start + answer_end**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHPxRRHuAnSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for train data\n",
        "y_train = []\n",
        "span_ofr = 0;\n",
        "params['train_span_outofrange'] = 0\n",
        "params['test_span_outofrange'] = 0\n",
        "params['val_span_outofrange'] = 0\n",
        "\n",
        "for i in range(len(train)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    start, end = train[\"answer_word_span\"].iloc[i]\n",
        "    # if(start < params['context_length'] and end < params['context_length']):\n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    # else:\n",
        "    #   span_ofr = span_ofr + 1\n",
        "    #   print(start,end)\n",
        "    y_train.append(np.concatenate((s,e)))    \n",
        "\n",
        "params['train_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "\n",
        "# for test data\n",
        "y_test = []\n",
        "for i in range(len(test)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = test[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    y_test.append(np.concatenate((s,e)))\n",
        "\n",
        "params['test_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "                \n",
        "# for val data\n",
        "y_val = []\n",
        "for i in range(len(val)):\n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = val[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1      \n",
        "    y_val.append(np.concatenate((s,e)))\n",
        "\n",
        "params['val_span_outofrange'] = span_ofr    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgPgnMl0VZCd",
        "colab_type": "code",
        "outputId": "bea9c5d6-d7af-48a3-f25c-5558cd31b285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(y_train),len(y_train[0]))\n",
        "print(len(y_test),len(y_test[0]))\n",
        "print(len(y_val),len(y_val[0]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78183 852\n",
            "26062 852\n",
            "26061 852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rZCyBydBZW1",
        "colab_type": "code",
        "outputId": "4fd959fd-ed74-4948-ef84-b959056075bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "pprint.pprint(params)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkTaGKrnLLFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of context word length = 285\n",
        "# 99% percentile or question word lengt = 20\n",
        "max_context_seq_length = 285\n",
        "max_question_seq_length = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDzXjXa3MpP0",
        "colab_type": "code",
        "outputId": "309c4f8e-3eba-4dc5-852c-ffebd142cceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(squad_df['clean_context'][2000])\n",
        "print(context_sequence[3000])\n",
        "print(squad_df['clean_question'][2000])\n",
        "print(questions_sequence[2000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "october 21 2008 apple reported 14 21 total revenue fiscal quarter 4 year 2008 came ipods september 9 2009 keynote presentation apple event phil schiller announced total cumulative sales ipods exceeded 220 million continual decline ipod sales since 2009 surprising trend apple corporation apple cfo peter oppenheimer explained june 2009 expect traditional mp3 players decline time cannibalize ipod touch iphone since 2009 companys ipod sales continually decreased every financial quarter 2013 new model introduced onto market\n",
            "[384, 511, 58838, 520, 504, 34, 7655, 7507, 1434, 2740, 3028, 8, 22, 1434, 5566, 48288, 1070, 2828, 4793, 266, 1211, 9403, 2828, 5566, 7837, 3016, 1636, 4027, 697, 1922, 5566, 252, 2271, 4793, 27639, 99, 578, 5016, 142, 1434, 680, 5683, 142, 5566, 7, 58839, 1562, 11769, 4284, 17681, 255, 58840, 1050, 3760, 5209, 24179, 1084, 2740, 3028, 235, 1376, 115, 6740, 179, 1578, 374, 4793, 27639, 578, 8588, 44, 3439, 19, 455, 2820, 574, 193, 578, 87, 13, 11280, 60, 3366, 636]\n",
            "who was chief financial officer of apple in july of 2009\n",
            "[37, 11, 767, 520, 2272, 3, 859, 5, 335, 3, 281]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FSl5UDL_qD",
        "colab_type": "text"
      },
      "source": [
        "## 4 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS6fKEvieWrX",
        "colab_type": "text"
      },
      "source": [
        "**Implements a baseline 0 in Deep Learning based approach per our project synopsis. This baseline model uses the following layers **\n",
        "0.   Input layer\n",
        "1.   Embedding Layer\n",
        "2.   List LSTM\n",
        "3.   a custom Bilinear Similarity layer \n",
        "4.   Prediction Layer\n",
        "5.   Output layer \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9JFn3oWiU4y",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Building Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLGurD9eijTh",
        "colab_type": "text"
      },
      "source": [
        "**For Questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLqUQHSjeVwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4511761b-9100-448d-8f2e-15e2ec1a2557"
      },
      "source": [
        "# question embedding\n",
        "q_input = layers.Input(shape=(params['question_max_length'],),name=\"QUESTION_INPUT\")\n",
        "q_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
        "                  output_dim=params['embedding_size'],\n",
        "                  name=\"QUESTION_EMBEDDING\")(q_input)\n",
        "\n",
        "# encoder \n",
        "q_output = layers.LSTM(units=params['rnn_units'], \n",
        "                     name='QUESTION_LSTM')(q_emb)\n",
        "# q_cont = BiLSTM(UNITS)(q_emb)\n",
        "\n",
        "# question context\n",
        "# q_cont,_ = Attention()(q_cont) \n",
        "print(q_output.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eJ2ykC1megw",
        "colab_type": "text"
      },
      "source": [
        "**For Context**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glhG509P5Yh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29257b5b-9df5-4d01-d4ed-86f342026723"
      },
      "source": [
        "c_input = layers.Input(shape=(params['context_max_length'],),name=\"CONTEXT_INPUT\")\n",
        "\n",
        "# context embedding\n",
        "c_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
        "                  output_dim=params['embedding_size'],\n",
        "                  name=\"CONTEXT_EMBEDDING\")(c_input)\n",
        "\n",
        "\n",
        "\n",
        "# exact_match\n",
        "# ex_ = Input(shape=(CON_LEN,2))\n",
        "\n",
        "# # pos tags\n",
        "# pos_ = Input(shape=(CON_LEN,len(tag_to_num)+1))\n",
        "\n",
        "# # term frequency\n",
        "# term_ = Input(shape=(CON_LEN,1)) \n",
        "\n",
        "# concatenate input\n",
        "# concat = concatenate([c_emb,ex_,pos_,term_])\n",
        "\n",
        "c_output = layers.LSTM(params['rnn_units'],\n",
        "                 name='CONTEXT_LSTM',return_sequences=True)(c_emb)\n",
        "\n",
        "print(\"final output to bilinear \",c_output.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final output to bilinear  (None, 426, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g93054zrp9yp",
        "colab_type": "text"
      },
      "source": [
        "**Bilinear Term**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsrWO_tpppa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "469a3732-d1d4-4ade-afc9-e555d3131422"
      },
      "source": [
        "# bilinear term ####\n",
        "print(\"Question context shape \",q_output.shape)\n",
        "print(\"final o/p of context \",c_output.shape)\n",
        "\n",
        "################ start prediction ######################\n",
        "start = layers.Dense(params['rnn_units'])(q_output)\n",
        "\n",
        "# ading time_slice to question (batch_size,1,hidden)\n",
        "# shape (64,128) --> (64,1,128)\n",
        "hidden_start_time_axis = tf.expand_dims(start, -1)\n",
        "\n",
        "# squeeze remooves time slice we added before\n",
        "# final shape = (batch_size,decoder_timesteps)\n",
        "start_ = tf.squeeze(tf.matmul(c_output,hidden_start_time_axis),-1)\n",
        "    \n",
        "start_ = tf.nn.softmax(start_,axis = 1)\n",
        "    \n",
        "################ end prediction ######################\n",
        "end = layers.Dense(params['rnn_units'])(q_output)\n",
        "\n",
        "hidden_end_time_axis = tf.expand_dims(end, -1)\n",
        "\n",
        "# squeeze remooves time slice we added before\n",
        "# final shape = (batch_size,decoder_timesteps)\n",
        "end_ = tf.squeeze(tf.matmul(c_output,hidden_end_time_axis),-1)\n",
        "end_ = tf.nn.softmax(end_,axis=1)\n",
        "\n",
        "prob_token_span = tf.concat((start_,end_),axis = 1)\n",
        "print(\"Logits shape \",prob.shape)\n",
        "\n",
        "####### Prediction ### \n",
        "token_span = 20\n",
        "start_prob = prob[:,:params['context_max_length']]\n",
        "end_prob = prob[:,params['context_max_length']:]\n",
        "\n",
        "# do the outer product\n",
        "outer = tf.matmul(tf.expand_dims(start_prob, axis=2),tf.expand_dims(end_prob, axis=1))\n",
        "\n",
        "outer = tf.linalg.band_part(outer, 0, token_span)\n",
        "\n",
        "#print(outer.shape)\n",
        "\n",
        "# start_position will have shape of (batch_size,)\n",
        "start_position = tf.reduce_max(outer, axis=2)\n",
        "#end position will have shape of (batch_size,)\n",
        "end_position = tf.reduce_max(outer, axis=1)\n",
        "\n",
        "y_probab = tf.concat([start_position,end_position],axis=1)\n",
        "\n",
        "\n",
        "# logits = BilinearSimilarity(UNITS)(q_cont,c_)\n",
        "# Y_prob = Prediction()(logits)\n",
        "# print(\"Logits shape \",logits.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question context shape  (None, 256)\n",
            "final o/p of context  (None, 426, 256)\n",
            "Logits shape  (None, 852)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv9aw2EewJbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58d72b86-5426-46af-81f1-cd18a8690f42"
      },
      "source": [
        "model = Model(inputs = [q_input,c_input],outputs =y_probab)\n",
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "QUESTION_INPUT (InputLayer)     [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_EMBEDDING (Embedding)  (None, 40, 100)      10085100    QUESTION_INPUT[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_INPUT (InputLayer)      [(None, 426)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_LSTM (LSTM)            (None, 256)          365568      QUESTION_EMBEDDING[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_EMBEDDING (Embedding)   (None, 426, 100)     10085100    CONTEXT_INPUT[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_LSTM (LSTM)             (None, 426, 256)     365568      CONTEXT_EMBEDDING[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_2 (Tenso [(None, 256, 1)]     0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_3 (Tenso [(None, 256, 1)]     0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_2 (Te [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_ExpandDims_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_3 (Te [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_ExpandDims_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_2 (TensorFl [(None, 426)]        0           tf_op_layer_BatchMatMulV2_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_3 (TensorFl [(None, 426)]        0           tf_op_layer_BatchMatMulV2_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softmax_2 (TensorFl [(None, 426)]        0           tf_op_layer_Squeeze_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softmax_3 (TensorFl [(None, 426)]        0           tf_op_layer_Squeeze_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1 (TensorFlo [(None, 852)]        0           tf_op_layer_Softmax_2[0][0]      \n",
            "                                                                 tf_op_layer_Softmax_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [(None, 426)]        0           tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te [(None, 426)]        0           tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_12 (Tens [(None, 426, 1)]     0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_13 (Tens [(None, 1, 426)]     0           tf_op_layer_strided_slice_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_11 (T [(None, 426, 426)]   0           tf_op_layer_ExpandDims_12[0][0]  \n",
            "                                                                 tf_op_layer_ExpandDims_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MatrixBandPart_1 (T [(None, 426, 426)]   0           tf_op_layer_BatchMatMulV2_11[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Max_2 (TensorFlowOp [(None, 426)]        0           tf_op_layer_MatrixBandPart_1[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Max_3 (TensorFlowOp [(None, 426)]        0           tf_op_layer_MatrixBandPart_1[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_6 (TensorFlo [(None, 852)]        0           tf_op_layer_Max_2[0][0]          \n",
            "                                                                 tf_op_layer_Max_3[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 21,032,920\n",
            "Trainable params: 21,032,920\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}