{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/84/23ed6a1796480a6f1a2d38f2802901d078266bda38388954d01d3f2e821d/pip-20.1.1-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 11.9MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "Successfully installed pip-20.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 25 kB/s s eta 0:00:01     |██████▉                         | 109.8 MB 17.5 MB/s eta 0:00:24     |█████████▋                      | 154.3 MB 49.4 MB/s eta 0:00:08     |█████████████████████████████▊  | 479.4 MB 57.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Using cached tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/lib/python3/dist-packages (from tensorflow-gpu) (0.30.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.24.0)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.9.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (41.2.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0.post3)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.4.5.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
      "\u001b[31mERROR: tensorboard 2.2.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.24.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow-gpu\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.0.2\n",
      "    Uninstalling tensorboard-2.0.2:\n",
      "      Successfully uninstalled tensorboard-2.0.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.0.1\n",
      "    Uninstalling tensorflow-estimator-2.0.1:\n",
      "      Successfully uninstalled tensorflow-estimator-2.0.1\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "  Attempting uninstall: tensorflow-gpu\n",
      "    Found existing installation: tensorflow-gpu 2.0.0\n",
      "    Uninstalling tensorflow-gpu-2.0.0:\n",
      "      Successfully uninstalled tensorflow-gpu-2.0.0\n",
      "Successfully installed gast-0.3.3 tensorboard-2.2.2 tensorflow-estimator-2.2.0 tensorflow-gpu-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow-gpu\n",
      "Version: 2.2.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python3.6/dist-packages\n",
      "Requires: opt-einsum, termcolor, numpy, google-pasta, protobuf, grpcio, wheel, h5py, scipy, wrapt, tensorflow-estimator, gast, absl-py, astunparse, tensorboard, six, keras-preprocessing\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade tensorflow\n",
    "!pip show tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "DA8sHlEbfYnn",
    "outputId": "95b178aa-d400-4edd-ea0a-c84aad5c5afe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-1.9.3-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-httplib2\n",
      "  Downloading google_auth_httplib2-0.0.3-py2.py3-none-any.whl (6.3 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.46.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth>=1.16.0\n",
      "  Downloading google_auth-1.17.2-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/lib/python3/dist-packages (from google-api-python-client) (1.11.0)\n",
      "Collecting google-api-core<2dev,>=1.18.0\n",
      "  Downloading google_api_core-1.20.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httplib2<1dev,>=0.9.2\n",
      "  Downloading httplib2-0.18.1-py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 8.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client) (41.2.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client) (3.9.2)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 34.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 16.4 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 47.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 60.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client) (2.6)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.4.5.2-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 47.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 53.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyasn1, pyasn1-modules, rsa, cachetools, google-auth, pytz, urllib3, certifi, chardet, requests, googleapis-common-protos, google-api-core, httplib2, uritemplate, google-auth-httplib2, google-api-python-client, oauthlib, requests-oauthlib, google-auth-oauthlib, tqdm\n",
      "Successfully installed cachetools-4.1.0 certifi-2020.4.5.2 chardet-3.0.4 google-api-core-1.20.0 google-api-python-client-1.9.3 google-auth-1.17.2 google-auth-httplib2-0.0.3 google-auth-oauthlib-0.4.1 googleapis-common-protos-1.52.0 httplib2-0.18.1 oauthlib-3.1.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytz-2020.1 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.6 tqdm-4.46.1 uritemplate-3.0.1 urllib3-1.25.9\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.46.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 50.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-0.15.1-py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 57.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2020.6.8-cp36-cp36m-manylinux2010_x86_64.whl (660 kB)\n",
      "\u001b[K     |████████████████████████████████| 660 kB 49.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.46.1)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1435513 sha256=8b2d3b3fc4c04920ceba5557668d56756fe2091af0f17a806834bc2793a04149\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "Successfully built nltk\n",
      "Installing collected packages: joblib, threadpoolctl, scikit-learn, click, regex, nltk\n",
      "Successfully installed click-7.1.2 joblib-0.15.1 nltk-3.5 regex-2020.6.8 scikit-learn-0.23.1 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyDrive\n",
      "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
      "\u001b[K     |████████████████████████████████| 987 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.9.3)\n",
      "Collecting oauth2client>=4.0.0\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 16.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML>=3.0\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 51.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.18.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.20.0)\n",
      "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/lib/python3/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client>=1.2->PyDrive) (3.9.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client>=1.2->PyDrive) (1.52.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client>=1.2->PyDrive) (2020.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client>=1.2->PyDrive) (41.2.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.1.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client>=1.2->PyDrive) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client>=1.2->PyDrive) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client>=1.2->PyDrive) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.18.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n",
      "Building wheels for collected packages: PyDrive, PyYAML\n",
      "  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDrive: filename=PyDrive-1.3.1-py3-none-any.whl size=25756 sha256=105f1972016d6b75afcfa27ff60d03ff5d48fedb9c5ed2bba03f345939ad9803\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/8f/dd/be434b75e07ab06eaa16cbaf7d3e03d8ad68c12929ec3022a4\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=45919 sha256=8f32ab535219bf79c4a8effeb42fed27b4d1cd3a23d6f82e1ff5017a2f6de5d1\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
      "Successfully built PyDrive PyYAML\n",
      "Installing collected packages: oauth2client, PyYAML, PyDrive\n",
      "Successfully installed PyDrive-1.3.1 PyYAML-5.3.1 oauth2client-4.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.0.4-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "879mz4G6-lRH"
   },
   "source": [
    "## 1. Import Libraries, setting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=607263109331-gaed96n8hjeqf7er45au9ifj0834geog.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/0wHO1MiefKpbnGWzxohzqqb2tba9IP99WK98XWg8Cp3xnYhmKwX2KBQ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.CommandLineAuth()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/open?id=1_7Vm3CSZiK0QEKjJ4R1lYY-rHAMltTN2\n",
    "download = drive.CreateFile({'id': '1HeKbRZhCq5kxkSb7g1E6XNY2ZAecCiUN'})\n",
    "download.GetContentFile('squad_data_final_context_withoutstopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/open?id=1TbjOhYtXTnTdyM31YiZGEK4Jp49xN__s\n",
    "# embedding\n",
    "download = drive.CreateFile({'id': '1TbjOhYtXTnTdyM31YiZGEK4Jp49xN__s'})\n",
    "download.GetContentFile('glove300dembedmatrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/open?id=1I6sf38YKJBc_UqXJH5iKHSEJwcczkVTt\n",
    "# tokenizer  \n",
    "download = drive.CreateFile({'id': '1I6sf38YKJBc_UqXJH5iKHSEJwcczkVTt'})\n",
    "download.GetContentFile('tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u_po1L9T-V5d",
    "outputId": "298f62f2-eedb-4dd9-fa3f-d84025a3f442"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "yTDFtsEtV7dY",
    "outputId": "06e3fab7-3e54-4397-af6a-52ba4c6b3dae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "import pickle\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint\n",
    "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Dropout,BatchNormalization,Flatten,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from numpy import array\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uPD8_fgd8PhR"
   },
   "outputs": [],
   "source": [
    "# we will store the params as we go along in this object\n",
    "params = {}\n",
    "project_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/Squad2.0/TrainingDataset/\"\n",
    "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\"\n",
    "tensorboard_logpath  = \"/content/drive/My Drive/AIML-MRC-Capstone/models/tensorboard-logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for paperspace\n",
    "# we will store the params as we go along in this object\n",
    "params = {}\n",
    "project_path = \"/storage/\"\n",
    "model_path = \"/storage/models/\"\n",
    "tensorboard_logpath  = \"/notebooks/tensorboard-logs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "09eGk3oWWCaP"
   },
   "source": [
    "# Objective - LSTM Baseline 0 \n",
    "\n",
    "*   **Inputs: A question q = {q1, ..., qQ} of length Q and a context paragraph p = {p1, ..., pP } of length P.**\n",
    "*   **Output: An answer span {as, ae} where as is the index of the first answer token in p, ae is the index of the last answer token in p, 0 <= as, ae >= m, and ae >= as.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-TACUiuOCJw"
   },
   "source": [
    "## 0 Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtBcy2Qwm2-c"
   },
   "source": [
    "#### 0.1 Custom function for preprocessing of context and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LFr3-S_Gm9FX"
   },
   "outputs": [],
   "source": [
    "# remove unwanted chars\n",
    "# convert to lowercase\n",
    "# remove unwanted spaces\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "## reference \n",
    "def decontracted(phrase):\n",
    "    \"\"\"\n",
    "    This function remooves punctuation from given sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    if(phrase is np.nan):\n",
    "      return 'impossible'      \n",
    "\n",
    "    try:      \n",
    "      # specific\n",
    "      phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "      phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "      # general\n",
    "      phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "      phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "      phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "      phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "      phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "      phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "      phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "      phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "      \n",
    "      # string operation\n",
    "      phrase = phrase.replace('\\\\r', ' ')\n",
    "      phrase = phrase.replace('\\\\\"', ' ')\n",
    "      phrase = phrase.replace('\\\\n', ' ')\n",
    "\n",
    "      phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase.lower())\n",
    "    except:\n",
    "      print(phrase)  \n",
    "    \n",
    "    return phrase\n",
    "\n",
    "def preprocess_text(corpus, text_lower_case=True, \n",
    "                      special_char_removal=True, stopword_removal=True, remove_digits=False):    \n",
    "    normalized_text = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # doc = decontracted(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits) \n",
    "\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc)\n",
    "\n",
    "        normalized_text.append(doc)\n",
    "        \n",
    "    return normalized_text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    #Using regex\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):  \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)                 \n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20-D8HBeOf_p"
   },
   "source": [
    "### 0.2 Answer Span from Context and Answer, and reverse for predicted spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbM8z2AEjxKK"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Returns tokenised words.\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def answer_span(context,ans):\n",
    "    \"\"\"\n",
    "    This funtion returns anwer span start index and end index.\n",
    "    \"\"\"\n",
    "    ans_token = tokenize(ans)\n",
    "    con_token = tokenize(context)\n",
    "    ans_len = len(ans_token)\n",
    "    \n",
    "    if ans_len!=0 and ans_token[0] in con_token:\n",
    "    \n",
    "        indices = [i for i, x in enumerate(con_token) if x == ans_token[0]]        \n",
    "        try:\n",
    "\n",
    "            if(len(indices)>1):\n",
    "                start = [i for i in indices if (con_token[i:i+ans_len] == ans_token) ]\n",
    "                end = start[0] + ans_len - 1\n",
    "                return start[0],end\n",
    "\n",
    "            else:\n",
    "                start = con_token.index(ans_token[0])\n",
    "                end = start + ans_len - 1\n",
    "                return start,end\n",
    "        except:\n",
    "            return -1,-1\n",
    "    else:\n",
    "        return -1,-1\n",
    "\n",
    "def span_to_answer(span, context):\n",
    "  con_token = tokenize(context)  \n",
    "  return ' '.join(con_token[span[0]:span[1]+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sveyxKK5j8q"
   },
   "source": [
    "### 0.3 Update and persist params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K170rfN15qGP"
   },
   "outputs": [],
   "source": [
    "### SAVE PARAMS\n",
    "# Writing to sample.json \n",
    "\n",
    "def updateparams():\n",
    "  with open(model_path + \"params.json\", \"w\") as p: \n",
    "    p.write(json.dumps(params))\n",
    "  print(\"params.jsop updated and can be found in \", model_path + \"params.json\")  \n",
    "\n",
    "# updateparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYMDmMJNU9mw"
   },
   "outputs": [],
   "source": [
    "def showparams():\n",
    "  pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxuHGmpRMBHT"
   },
   "source": [
    "## 1 Context, Answer EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tWRDHA6z_Ji"
   },
   "source": [
    "**<font color=\"red\">BE CAREFUL BEFORE EXECUTING THIS PLEASE. THERE IS HIGH CHANCE THAT THIS WILL OVERWRITE EXISTING DATAFRAMES</font>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsuZ3NdiMOiB"
   },
   "outputs": [],
   "source": [
    "squad_df = pd.read_csv(project_path+'squad_data_final_withstopword_withpunctuation.csv')\n",
    "squad_df.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vps2_zIB7q7i"
   },
   "outputs": [],
   "source": [
    "### specific cleaning of context and question\n",
    "### DO NOT REMOVE STOP WORDS \n",
    "###\n",
    "squad_df['clean_context'] = preprocess_text(squad_df['context'],stopword_removal=False, special_char_removal=False)\n",
    "squad_df['clean_question'] = preprocess_text(squad_df['question'],stopword_removal=False, special_char_removal=False)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZ5CD4fzCPcj"
   },
   "outputs": [],
   "source": [
    "squad_df['clean_answer'] = preprocess_text(squad_df['answer'],stopword_removal=False, special_char_removal = False)\n",
    "\n",
    "# preprocess_text([squad_df['answer'].iloc[23]],stopword_removal=False, special_char_removal = False)\n",
    "# preprocess_text([np.nan],stopword_removal=False, special_char_removal = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "UgrKpCLgC8q-",
    "outputId": "381d3afc-8e27-42fc-9cf4-2c67356096d3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'squad_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7edcac59cafe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msquad_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'squad_df' is not defined"
     ]
    }
   ],
   "source": [
    "squad_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jhUog7nH_uqL"
   },
   "outputs": [],
   "source": [
    "ans_span = []\n",
    "for i in range(len(squad_df)):\n",
    "    s,e = answer_span(squad_df[\"clean_context\"].iloc[i],squad_df[\"clean_answer\"].iloc[i])\n",
    "    ans_span.append((s,e))\n",
    "\n",
    "squad_df[\"answer_word_span\"] = ans_span    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "BpMnsnfCIqh1",
    "outputId": "664eb3b0-179e-4063-c638-4deb57068883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 16)\n",
      "(43502, 16)\n",
      "No of records which does not have answer but span not found in context =  (0, 16)\n"
     ]
    }
   ],
   "source": [
    "# check no of right answer span detection \n",
    "print(squad_df[squad_df[\"answer_word_span\"] == (-1,-1)].shape)\n",
    "print(squad_df[squad_df['clean_answer'] == 'impossible' ].shape)\n",
    "\n",
    "print('No of records which does not have answer but span not found in context = ', \n",
    "      squad_df[(squad_df['clean_answer'] != 'impossible') & (squad_df[\"answer_word_span\"] == (-1,-1))].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XK2rwLdg9Aqq"
   },
   "outputs": [],
   "source": [
    "# write the latest greatet\n",
    "squad_df.to_csv(project_path+'squad_data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYRTJ6qX9kRj"
   },
   "outputs": [],
   "source": [
    "squad_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "ltvYXgi07_p2",
    "outputId": "d373fc87-1564-427d-d301-2f2392144c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('beyonc giselle knowles was born in houston texas to celestine ann tina '\n",
      " 'knowles n e beyinc a hairdresser and salon owner and mathew knowles a xerox '\n",
      " 'sales manager beyonc is name is a tribute to her mother is maiden name '\n",
      " 'beyonc is younger sister solange is also a singer and a former member of '\n",
      " 'destiny is child mathew is african american while tina is of louisiana '\n",
      " 'creole descent with african native american french cajun and distant irish '\n",
      " 'and spanish ancestry through her mother beyonc is a descendant of acadian '\n",
      " 'leader joseph broussard she was raised in a methodist household ')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(squad_df['clean_context'].iloc[39])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGZp4iLEHE8n"
   },
   "source": [
    "## 2 Load Squad Data - Cleaned and curated (output of preprocessing step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFPpoxl7Afls"
   },
   "source": [
    "### 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "4_u1n8G3fge_",
    "outputId": "30c9d865-f97b-468a-f7f5-bf49db9150a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 130306 entries, 0 to 130305\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   title                   130306 non-null  object \n",
      " 1   context                 130306 non-null  object \n",
      " 2   question                130306 non-null  object \n",
      " 3   id                      130306 non-null  object \n",
      " 4   answer_start            130306 non-null  int64  \n",
      " 5   answer                  86807 non-null   object \n",
      " 6   plausible_answer_start  43498 non-null   float64\n",
      " 7   plausible_answer        43498 non-null   object \n",
      " 8   is_impossible           130306 non-null  bool   \n",
      " 9   clean_context           130306 non-null  object \n",
      " 10  clean_question          130306 non-null  object \n",
      " 11  clean_answer            130306 non-null  object \n",
      " 12  answer_len              130306 non-null  int64  \n",
      " 13  answer_end              130306 non-null  int64  \n",
      " 14  answer_span             130306 non-null  object \n",
      " 15  answer_word_span        130306 non-null  object \n",
      "dtypes: bool(1), float64(1), int64(3), object(11)\n",
      "memory usage: 15.0+ MB\n",
      "None\n",
      "beyonc giselle knowlescarter bijnse beeyonsay born september 4 1981 american singer songwriter record producer actress born raised houston texas performed various singing dancing competitions child rose fame late 1990s lead singer rb girlgroup destinys child managed father mathew knowles group became one worlds bestselling girl groups time hiatus saw release beyoncs debut album dangerously love 2003 established solo artist worldwide earned five grammy awards featured billboard hot 100 numberone singles crazy love baby boy\n"
     ]
    }
   ],
   "source": [
    "#### NOTE THE 2 data frames's\n",
    "df_nostopwords = 'squad_data_final_context_withoutstopwords.csv'\n",
    "# df_withstopwords = 'squad_data_final_withstopword_withpunctuation.csv'\n",
    "squad_df = pd.read_csv(project_path+'squad_data_final_context_withoutstopwords.csv')\n",
    "squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "squad_df[\"answer_word_span\"] = squad_df[\"answer_word_span\"].apply(lambda x :eval(x))\n",
    "print(squad_df.info())\n",
    "print(squad_df['clean_context'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "uJ_KUPvezIOi",
    "outputId": "63593c23-5894-4abe-931f-8e11660e3eaf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer</th>\n",
       "      <th>plausible_answer_start</th>\n",
       "      <th>plausible_answer</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>clean_context</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_answer</th>\n",
       "      <th>answer_len</th>\n",
       "      <th>answer_end</th>\n",
       "      <th>answer_span</th>\n",
       "      <th>answer_word_span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>269</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
       "      <td>when did beyonce start becoming popular</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>17</td>\n",
       "      <td>286</td>\n",
       "      <td>(269, 286)</td>\n",
       "      <td>(-1, -1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>207</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
       "      <td>what areas did beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>19</td>\n",
       "      <td>226</td>\n",
       "      <td>(207, 226)</td>\n",
       "      <td>(21, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>526</td>\n",
       "      <td>2003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
       "      <td>when did beyonce leave destinys child and beco...</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "      <td>530</td>\n",
       "      <td>(526, 530)</td>\n",
       "      <td>(55, 55)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     title                                            context  \\\n",
       "0  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question  \\\n",
       "0           When did Beyonce start becoming popular?   \n",
       "1  What areas did Beyonce compete in when she was...   \n",
       "2  When did Beyonce leave Destiny's Child and bec...   \n",
       "\n",
       "                         id  answer_start               answer  \\\n",
       "0  56be85543aeaaa14008c9063           269    in the late 1990s   \n",
       "1  56be85543aeaaa14008c9065           207  singing and dancing   \n",
       "2  56be85543aeaaa14008c9066           526                 2003   \n",
       "\n",
       "   plausible_answer_start plausible_answer  is_impossible  \\\n",
       "0                     NaN              NaN          False   \n",
       "1                     NaN              NaN          False   \n",
       "2                     NaN              NaN          False   \n",
       "\n",
       "                                       clean_context  \\\n",
       "0  beyonc giselle knowlescarter bijnse beeyonsay ...   \n",
       "1  beyonc giselle knowlescarter bijnse beeyonsay ...   \n",
       "2  beyonc giselle knowlescarter bijnse beeyonsay ...   \n",
       "\n",
       "                                      clean_question         clean_answer  \\\n",
       "0            when did beyonce start becoming popular    in the late 1990s   \n",
       "1  what areas did beyonce compete in when she was...  singing and dancing   \n",
       "2  when did beyonce leave destinys child and beco...                 2003   \n",
       "\n",
       "   answer_len  answer_end answer_span answer_word_span  \n",
       "0          17         286  (269, 286)         (-1, -1)  \n",
       "1          19         226  (207, 226)         (21, 23)  \n",
       "2           4         530  (526, 530)         (55, 55)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U35P3ZvGAQQD"
   },
   "source": [
    "### 2.2 Create Train, Validation and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "7M9hW4xy-Suj",
    "outputId": "66327e5f-b463-4216-d034-60642f1ad3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78183, 16)\n",
      "(26061, 16)\n",
      "(26062, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample,shuffle\n",
    "\n",
    "# train = resample(train)\n",
    "# train = shuffle(train,n_samples =50000)\n",
    "\n",
    "train,test = train_test_split(squad_df,test_size = 0.2)\n",
    "train,val = train_test_split(train,test_size=0.25)\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6dcPCGiJrz1"
   },
   "source": [
    "### 2.3 Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hxoMIrC6EIgS",
    "outputId": "6de74dee-3720-4dab-a610-1b21afd59edf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# params['tokenizer_num_words'] = 80000\n",
    "# tokenizer = preprocessing.text.Tokenizer(num_words=params['tokenizer_num_words'])\n",
    "\n",
    "# # NOTE: tokenizer is been made out of original dataset\n",
    "# for text in tqdm([squad_df['clean_context'], squad_df['clean_question']]):  \n",
    "#   tokenizer.fit_on_texts(text.values)\n",
    "\n",
    "# # total tokenizer words\n",
    "# params['vocab_size'] = len(tokenizer.word_index)\n",
    "\n",
    "# ### SAVE TOKENIZERS\n",
    "# with open(model_path + \"tokenizer.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5FJHi14rHNTX",
    "outputId": "c75e67a7-f9e2-40fd-ea66-5efa45b4eb94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100850"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(model_path + \"tokenizer.pkl\",\"rb\") as infile:\n",
    "    tokenizer = pickle.load(infile)\n",
    "\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y7fIbj3tatHH",
    "outputId": "4d687c1c-ee99-4b7e-9322-a1e32232f635"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['how']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gXSy_y36IFb"
   },
   "source": [
    "### 2.4 Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "2whUHcmX5PwS",
    "outputId": "0934a096-df55-437a-bb9e-5420160e59f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_length_99': 285,\n",
      " 'context_pad_seq': 'pre',\n",
      " 'embedding_size': 300,\n",
      " 'question_length_99': 20,\n",
      " 'question_pad_seq': 'pre',\n",
      " 'rnn_units': 256,\n",
      " 'test_shape': (26062, 16),\n",
      " 'train_shape': (78183, 16),\n",
      " 'val_shape': (26061, 16),\n",
      " 'vocab_size': 100850}\n"
     ]
    }
   ],
   "source": [
    "# From the EDA and historgrams we can conclude that - \n",
    "# 99% percentile of context word length = 285\n",
    "# 99% percentile or question word lengt = 20\n",
    "context_length = 285\n",
    "question_length = 20\n",
    "params['train_shape'] = train.shape\n",
    "params['val_shape'] = val.shape\n",
    "params['test_shape'] = test.shape\n",
    "params['context_length_99'] = context_length # initialize with a high percentile\n",
    "params['question_length_99'] = question_length # initialize with a high percentile\n",
    "params['embedding_size'] = 300\n",
    "params['rnn_units'] = 256\n",
    "params['context_pad_seq'] = 'pre'\n",
    "params['question_pad_seq'] = 'pre'\n",
    "params['vocab_size'] = len(tokenizer.word_index)\n",
    "\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lRuVtp_7y51"
   },
   "source": [
    "## 3 Vectorization / Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEJqzOfW9ARO"
   },
   "source": [
    "#### 3.1 Integer Sequence of Context and Question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhuvjYmZ7m6W"
   },
   "outputs": [],
   "source": [
    "train_clean_context_sequence = tokenizer.texts_to_sequences(train[\"clean_context\"].values)\n",
    "test_clean_context_sequence = tokenizer.texts_to_sequences(test[\"clean_context\"].values)\n",
    "val_clean_context_sequence = tokenizer.texts_to_sequences(val[\"clean_context\"].values)\n",
    "\n",
    "\n",
    "train_clean_question_sequence = tokenizer.texts_to_sequences(train[\"clean_question\"].values)\n",
    "test_clean_question_sequence = tokenizer.texts_to_sequences(test[\"clean_question\"].values)\n",
    "val_clean_question_sequence = tokenizer.texts_to_sequences(val[\"clean_question\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_clean_context_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "vXQNaAxehrDa",
    "outputId": "926da62b-606a-4b9a-ea1c-2a8b05d01f52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[93, 370, 16, 495, 1972, 1970, 35250, 59, 180, 71, 7457, 1151, 1, 25],\n",
       " [5,\n",
       "  2,\n",
       "  945,\n",
       "  16,\n",
       "  2811,\n",
       "  288,\n",
       "  10,\n",
       "  7298,\n",
       "  2223,\n",
       "  1563,\n",
       "  7624,\n",
       "  6781,\n",
       "  20187,\n",
       "  138,\n",
       "  3520,\n",
       "  16156],\n",
       " [5, 2, 29, 16, 844, 4809, 2477, 188, 2842],\n",
       " [39, 170, 4673, 1198, 101, 1705, 726, 728, 173, 18],\n",
       " [71, 11, 6, 900]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean_question_sequence[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "0kmRjkqRlExD",
    "outputId": "999a9c81-fa9d-41f7-c88e-b2d37f429fa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107708    what is called when the direction of the elctr...\n",
       "6212      which fan favorite was unexpectedly eliminated...\n",
       "1056      what term describes the qualities of the relat...\n",
       "12538          alb comes from the word alb which means what\n",
       "57848                     where is byus main campus located\n",
       "Name: clean_question, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['clean_question'][5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQpM6dGtoGkP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u41PKZTFcwm"
   },
   "source": [
    "#### 3.2 Find Max Sequence length of Context and Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "Jwh8HGs0Fbp2",
    "outputId": "960e729d-3bb4-4210-ec1b-9e39d7e880de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_length_99': 285,\n",
      " 'context_max_length': 426,\n",
      " 'context_pad_seq': 'pre',\n",
      " 'embedding_size': 300,\n",
      " 'question_length_99': 20,\n",
      " 'question_max_length': 40,\n",
      " 'question_pad_seq': 'pre',\n",
      " 'rnn_units': 256,\n",
      " 'test_shape': (26062, 16),\n",
      " 'train_shape': (78183, 16),\n",
      " 'val_shape': (26061, 16),\n",
      " 'vocab_size': 100850}\n"
     ]
    }
   ],
   "source": [
    "# max length of context\n",
    "params['context_max_length'] = max(max(len(txt) for txt in train_clean_context_sequence),\n",
    "                                  max(len(txt) for txt in test_clean_context_sequence),\n",
    "                                  max(len(txt) for txt in val_clean_context_sequence))\n",
    "\n",
    "params['question_max_length'] = max(max(len(txt) for txt in train_clean_question_sequence),\n",
    "                                  max(len(txt) for txt in test_clean_question_sequence),\n",
    "                                  max(len(txt) for txt in val_clean_question_sequence))\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhnBwThFIA0H"
   },
   "source": [
    "#### 3.3 Padding of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gGvIf7xU9rIJ",
    "outputId": "cfbff078-d302-4247-e2e0-d8b61bec5ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78183, 426)\n",
      "(26062, 426)\n",
      "(26061, 426)\n"
     ]
    }
   ],
   "source": [
    "train_context_sequence = preprocessing.sequence.pad_sequences(train_clean_context_sequence,\n",
    "                                                              maxlen=params['context_max_length'],\n",
    "                                                              padding=params['context_pad_seq'])\n",
    "test_context_sequence = preprocessing.sequence.pad_sequences(test_clean_context_sequence,\n",
    "                                                             maxlen=params['context_max_length'],\n",
    "                                                             padding=params['context_pad_seq'])\n",
    "val_context_sequence = preprocessing.sequence.pad_sequences(val_clean_context_sequence,\n",
    "                                                            maxlen=params['context_max_length'],\n",
    "                                                            padding=params['context_pad_seq'])\n",
    "\n",
    "print(train_context_sequence.shape)\n",
    "print(test_context_sequence.shape)\n",
    "print(val_context_sequence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78183,)\n",
      "(26062,)\n",
      "(26061,)\n"
     ]
    }
   ],
   "source": [
    "train_context_sequence_nopad = np.array(train_clean_context_sequence)\n",
    "test_context_sequence_nopad = np.array(test_clean_context_sequence)\n",
    "val_context_sequence_nopad = np.array(val_clean_context_sequence)\n",
    "\n",
    "print(train_context_sequence_nopad.shape)\n",
    "print(test_context_sequence_nopad.shape)\n",
    "print(val_context_sequence_nopad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_context_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2NxpIGIb9p5T",
    "outputId": "448b4805-d196-4b99-bb14-bb13cb43082c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78183, 40)\n",
      "(26062, 40)\n",
      "(26061, 40)\n"
     ]
    }
   ],
   "source": [
    "train_question_sequence = preprocessing.sequence.pad_sequences(train_clean_question_sequence,\n",
    "                                                               maxlen=params['question_max_length'],\n",
    "                                                               padding=params['question_pad_seq'])\n",
    "test_question_sequence = preprocessing.sequence.pad_sequences(test_clean_question_sequence,\n",
    "                                                              maxlen=params['question_max_length'],\n",
    "                                                              padding=params['question_pad_seq'])\n",
    "val_question_sequence = preprocessing.sequence.pad_sequences(val_clean_question_sequence,\n",
    "                                                             maxlen=params['question_max_length'],\n",
    "                                                             padding=params['question_pad_seq'])\n",
    "\n",
    "print(train_question_sequence.shape)\n",
    "print(test_question_sequence.shape)\n",
    "print(val_question_sequence.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  37,  691, 3811,    3,   26, 6631,  939, 5908,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_question_sequence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgCKNbH6_-yd"
   },
   "source": [
    "#### 3.4 Create Answer Sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bv0yC_w8AF4x"
   },
   "source": [
    "Encode y_trues as big array consisting of ans_start + ans_end. This has to be used in loss function as well. We will use the answer_word_span feature\n",
    "\n",
    "**y_true = answer_start + answer_end**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHPxRRHuAnSH"
   },
   "outputs": [],
   "source": [
    "# for train data\n",
    "y_train = []\n",
    "span_ofr = 0;\n",
    "params['train_span_outofrange'] = 0\n",
    "params['test_span_outofrange'] = 0\n",
    "params['val_span_outofrange'] = 0\n",
    "\n",
    "for i in range(len(train)):    \n",
    "    s = np.zeros(params['context_max_length'],dtype = \"float32\")\n",
    "    e = np.zeros(params['context_max_length'],dtype = \"float32\")\n",
    "    start, end = train[\"answer_word_span\"].iloc[i]    \n",
    "    s[start] = 1\n",
    "    e[end] = 1\n",
    "    y_train.append(np.concatenate((s,e)))    \n",
    "\n",
    "params['train_span_outofrange'] = span_ofr\n",
    "span_ofr = 0;\n",
    "\n",
    "# for test data\n",
    "y_test = []\n",
    "for i in range(len(test)):    \n",
    "    s = np.zeros(params['context_max_length'],dtype = \"float32\")\n",
    "    e = np.zeros(params['context_max_length'],dtype = \"float32\")        \n",
    "    start,end = test[\"answer_word_span\"].iloc[i]    \n",
    "    s[start] = 1\n",
    "    e[end] = 1\n",
    "    y_test.append(np.concatenate((s,e)))\n",
    "\n",
    "params['test_span_outofrange'] = span_ofr\n",
    "span_ofr = 0;\n",
    "                \n",
    "# for val data\n",
    "y_val = []\n",
    "for i in range(len(val)):\n",
    "    s = np.zeros(params['context_max_length'],dtype = \"float32\")\n",
    "    e = np.zeros(params['context_max_length'],dtype = \"float32\")        \n",
    "    start,end = val[\"answer_word_span\"].iloc[i]    \n",
    "    s[start] = 1\n",
    "    e[end] = 1      \n",
    "    y_val.append(np.concatenate((s,e)))\n",
    "\n",
    "params['val_span_outofrange'] = span_ofr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WgPgnMl0VZCd",
    "outputId": "228178f2-b35c-4e15-c058-9f05cb74d2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78183 852\n",
      "26062 852\n",
      "26061 852\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train),len(y_train[0]))\n",
    "print(len(y_test),len(y_test[0]))\n",
    "print(len(y_val),len(y_val[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFuJMid-iT7a"
   },
   "source": [
    "### 3.5 Check 1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SXXhftC5kE8A",
    "outputId": "7d6d28ff-3b24-494c-fead-8f58e176f177"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seasons'"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 1\n",
    "answer_span(train['clean_context'].iloc[index],train['clean_answer'].iloc[index])\n",
    "span_to_answer((22,22),train['clean_context'].iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "fAXS9YCpiVkn",
    "outputId": "82c281c6-aace-4565-81db-662b6ed38f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ori Cont = \n",
      "('An exhibition game may also be used to settle a challenge, to provide '\n",
      " 'professional entertainment, to promote the sport, or to raise money for '\n",
      " 'charities. Several sports leagues hold all-star games to showcase their best '\n",
      " 'players against each other, while other exhibitions games may pit '\n",
      " 'participants from two different leagues or countries to unofficially '\n",
      " 'determine who would be the best in the world. International competitions '\n",
      " 'like the Olympic Games may also hold exhibition games as part of a '\n",
      " 'demonstration sport.')\n",
      "CLean Cont = \n",
      "('exhibition game may also used settle challenge provide professional '\n",
      " 'entertainment promote sport raise money charities several sports leagues '\n",
      " 'hold allstar games showcase best players exhibitions games may pit '\n",
      " 'participants two different leagues countries unofficially determine would '\n",
      " 'best world international competitions like olympic games may also hold '\n",
      " 'exhibition games part demonstration sport')\n",
      "Question =  What type of exhibition game showcases average players?\n",
      "Clean Question =  what type of exhibition game showcases average players\n",
      "Answer =  nan\n",
      "Clean Answer =  IMPOSSIBLE\n",
      "AS,AE =  (-1, -1)\n",
      "encoded  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Ori Cont = \")\n",
    "pprint.pprint(train['context'].iloc[index])\n",
    "print(\"CLean Cont = \")\n",
    "pprint.pprint(train['clean_context'].iloc[index])\n",
    "print('Question = ',train['question'].iloc[index])\n",
    "print('Clean Question = ',train['clean_question'].iloc[index])\n",
    "print('Answer = ',train['answer'].iloc[index])\n",
    "print('Clean Answer = ',train['clean_answer'].iloc[index])\n",
    "print('AS,AE = ',train['answer_word_span'].iloc[index])\n",
    "print(\"encoded \", y_train[index])\n",
    "print(span_to_answer([60,62],train['clean_context'].iloc[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "-rZCyBydBZW1",
    "outputId": "2d7409c4-b910-49ed-8b25-97a8a50abef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_length_99': 285,\n",
      " 'context_max_length': 426,\n",
      " 'context_pad_seq': 'pre',\n",
      " 'embedding_size': 300,\n",
      " 'question_length_99': 20,\n",
      " 'question_max_length': 40,\n",
      " 'question_pad_seq': 'pre',\n",
      " 'rnn_units': 256,\n",
      " 'test_shape': (26062, 16),\n",
      " 'test_span_outofrange': 0,\n",
      " 'tokenizer_num_words': 80000,\n",
      " 'train_shape': (78183, 16),\n",
      " 'train_span_outofrange': 0,\n",
      " 'val_shape': (26061, 16),\n",
      " 'val_span_outofrange': 0,\n",
      " 'vocab_size': 100850}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "mDzXjXa3MpP0",
    "outputId": "6148bd68-7cc1-4088-acda-0d448ac6f047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beyonc giselle knowlescarter bijnse beeyonsay born september 4 1981 american singer songwriter record producer actress born raised houston texas performed various singing dancing competitions child rose fame late 1990s lead singer rb girlgroup destinys child managed father mathew knowles group became one worlds bestselling girl groups time hiatus saw release beyoncs debut album dangerously love 2003 established solo artist worldwide earned five grammy awards featured billboard hot 100 numberone singles crazy love baby boy\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0   928   295  1835  1057   369  4338   611  1010   859   658   834\n",
      "  1057   111  3329    52  4206   281  6389   201 39281  1895  1968  5633\n",
      "  1746   380  1056   469   189  1059   259   111   681 12083   228   403\n",
      "   111   317    97 51099 12542  2262 27307  5948  2982  2227   381  3227\n",
      "  2262   380  1056   369   112   259   373    29    29   396  1057   112\n",
      "   112    52   924   859  9631    45   242  4247   761   380  1056   224\n",
      "   381   978 11792 10284   942 12874]\n",
      "what was the first album beyonc released as a solo artist\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    2  326  341  170  847 1047 4361]\n"
     ]
    }
   ],
   "source": [
    "print(squad_df['clean_context'][10])\n",
    "print(train_context_sequence[110])\n",
    "print(squad_df['clean_question'][10])\n",
    "print(train_question_sequence[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0TSjNkdXzuR"
   },
   "source": [
    "### 3.6 Create a common function to generate sequences (useful in prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sq97Vq4kXTST"
   },
   "outputs": [],
   "source": [
    "# function to generate sequences withg appropiate padding\n",
    "def generate_question_context_sequence(context, question):\n",
    "  question_seq = tokenizer.texts_to_sequences(question)\n",
    "  context_seq = tokenizer.texts_to_sequences(context)\n",
    "  question_seq = preprocessing.sequence.pad_sequences(question_seq,\n",
    "                                                      maxlen=params['question_max_length'],\n",
    "                                                     padding=params['question_pad_seq'])\n",
    "  context_seq = preprocessing.sequence.pad_sequences(context_seq,\n",
    "                                                     maxlen=params['context_max_length'],\n",
    "                                                    padding=params['question_pad_seq'])\n",
    "  return context_seq, question_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "nkpMiW1HaqrL",
    "outputId": "ca3d0d72-5dea-4a9b-ca47-d81438c05679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when did the wildcard round become a factor\n",
      "(1, 426) (1, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True]])"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train[\"clean_question\"].iloc[1])\n",
    "\n",
    "c='state among best prekindergarten education national institute early education research rated first united states regard standards quality access prekindergarten education 2004 calling model early childhood schooling high school dropout rate decreased 3 1 2 5 percent 2007 2008 oklahoma ranked among 18 states 3 percent less dropout rate 2004 state ranked 36th nation relative number adults high school diplomas though 85 2 percent highest rate among southern states'\n",
    "q='when did the wildcard round become a factor'\n",
    "cs,qs = generate_question_context_sequence([c],[q])\n",
    "print(cs.shape,qs.shape)\n",
    "train_question_sequence[1] == qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "nl55IUkVck82",
    "outputId": "2acadfbf-7963-4e34-fffe-00fc0dcd4698"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,    71,    16,     1, 22943,\n",
       "        2043,   147,    26,  1775], dtype=int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_question_sequence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wt51Se0SchIp",
    "outputId": "27b9b5db-dce4-47d8-be35-145d808e0394"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when did the wildcard round become a factor'"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6FSl5UDL_qD"
   },
   "source": [
    "## 4 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yS6fKEvieWrX"
   },
   "source": [
    "**Implements a baseline 0 in Deep Learning based approach per our project synopsis. This baseline model uses the following layers **\n",
    "0.   Input layer\n",
    "1.   Embedding Layer\n",
    "2.   List LSTM\n",
    "3.   a custom Bilinear Similarity layer \n",
    "4.   Prediction Layer\n",
    "5.   Output layer \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9JFn3oWiU4y"
   },
   "source": [
    "### 4.2 Building Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOTSZc8_UeU2"
   },
   "source": [
    "#### Common Function - CUDNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwjPKOsTUdgY"
   },
   "outputs": [],
   "source": [
    "# As per https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM, it will use CuDNN Lstm\n",
    "# if below params match\n",
    "# activation == tanh\n",
    "# recurrent_activation == sigmoid\n",
    "# recurrent_dropout == 0\n",
    "# unroll is False\n",
    "# use_bias is True\n",
    "# Inputs are not masked or strictly right padded.\n",
    "\n",
    "def createCUDNNLstm(units,return_sequences,dropout,name=''):\n",
    "  return layers.LSTM(units=units,\n",
    "                     return_sequences=return_sequences, \n",
    "                     name = name,\n",
    "                     activation='tanh',\n",
    "                     recurrent_activation='sigmoid',\n",
    "                     recurrent_dropout=0,\n",
    "                     dropout=dropout,\n",
    "                     unroll=False,\n",
    "                     use_bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tLGurD9eijTh"
   },
   "source": [
    "#### Load Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zZhcy-0imtOx",
    "outputId": "2f527a82-a80f-405c-f3a3-9d8be8295720"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100851, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((params['vocab_size']+1,300))\n",
    "\n",
    "with open(model_path + \"glove300dembedmatrix.pkl\",\"rb\") as f:\n",
    "  embedding_matrix=pickle.load(f)\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU/GPU detection - Paperspace specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psutil\n",
      "  Downloading psutil-5.7.0.tar.gz (449 kB)\n",
      "\u001b[K     |████████████████████████████████| 449 kB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: psutil\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.0-cp36-cp36m-linux_x86_64.whl size=279877 sha256=244810956b32648aef85748fc4b608c4271810ac4e1ac64160db3dac05e2e36a\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/d9/f2/b5620c01e9b3e858c6877b1045fda5b115cf7df6490f883382\n",
      "Successfully built psutil\n",
      "Installing collected packages: psutil\n",
      "Successfully installed psutil-5.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical cores: 12\n",
      "Total cores: 12\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(\"Physical cores:\", psutil.cpu_count(logical=False))\n",
    "print(\"Total cores:\", psutil.cpu_count(logical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "# print(\"Num GPUs Available: \", tf.config)\n",
    "print(tf.config.list_physical_devices('CPU'))\n",
    "print(tf.config.list_logical_devices('CPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UmhiPPKrpCFv"
   },
   "source": [
    "#### Create TF Mirror Strategy for Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g1TCqo4_pBkL",
    "outputId": "93144a3b-229d-41a0-d025-874184b9be16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x7f690231e9b0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9OmnOEmSI7Oy"
   },
   "source": [
    "#### Questions Bi-LSTM Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rLqUQHSjeVwq",
    "outputId": "179fbea7-62ef-48eb-e7bf-e1455d0bac22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256)\n"
     ]
    }
   ],
   "source": [
    "# question embedding\n",
    "with strategy.scope():\n",
    "  q_input = layers.Input(shape=(params['question_max_length'],),name=\"QUESTION_INPUT\")\n",
    "  q_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
    "                    output_dim=300,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False, mask_zero= True,\n",
    "                    name=\"QUESTION_EMBEDDING\")(q_input)\n",
    "\n",
    "  # encoder \n",
    "  q_output=layers.Bidirectional(createCUDNNLstm(units=params['rnn_units'],\n",
    "                                                return_sequences=False,\n",
    "                                                dropout=0.2),\n",
    "                                merge_mode='sum',\n",
    "                                name='QUESTION_LSTM')(q_emb)\n",
    "\n",
    "#q_output = layers.Bidirectional(LSTM(units=params['rnn_units'], \n",
    "                     #name='QUESTION_LSTM', merge_mode=concatenate))(q_emb)\n",
    "print(q_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2eJ2ykC1megw"
   },
   "source": [
    "#### Context Bi-LSTM Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-glhG509P5Yh",
    "outputId": "6775d0d9-b3dd-4be5-e958-8b664653c50e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final context output  (None, 426, 256)\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  c_input = layers.Input(shape=(params['context_max_length'],),name=\"CONTEXT_INPUT\")\n",
    "\n",
    "  # context embedding\n",
    "  c_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
    "                    output_dim=300,\n",
    "                    weights=[embedding_matrix],trainable=False, mask_zero= True,\n",
    "                    name=\"CONTEXT_EMBEDDING\")(c_input)\n",
    "\n",
    "  c_output = layers.Bidirectional(createCUDNNLstm(params['rnn_units'],\n",
    "                                                  return_sequences=True,\n",
    "                                                  dropout=0.2),\n",
    "                                  merge_mode='sum',\n",
    "                                  name='CONTEXT_LSTM')(c_emb)\n",
    "\n",
    "print(\"final context output \",c_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g93054zrp9yp"
   },
   "source": [
    "#### Bilinear Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "CNsrWO_tpppa",
    "outputId": "460cbee9-4e0c-421c-dc07-a0c7ac14a7a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question context shape  (None, 256)\n",
      "final o/p of context  (None, 426, 256)\n",
      "Probab shape  Tensor(\"BILINEAR_AS_AE_CONCAT:0\", shape=(None, 852), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Reference -- https://github.com/kellywzhang/reading-comprehension/blob/master/attention.py\n",
    "# bilinear term ####\n",
    "print(\"Question context shape \",q_output.shape)\n",
    "print(\"final o/p of context \",c_output.shape)\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "  ################ start prediction ######################\n",
    "  start = layers.Dense(params['rnn_units'],name=\"BILINEAR_AS_SPAN\")(q_output)\n",
    "  hidden_start_time_axis = tf.expand_dims(start, 2, name='BILINEAR_AS_ADD_DIM')\n",
    "\n",
    "  # squeeze remooves time slice we added before\n",
    "  # final shape = (batch_size,decoder_timesteps)\n",
    "  start_ = tf.squeeze(tf.matmul(c_output,hidden_start_time_axis,name=\"BILINEAR_AS_MATMUL_Q_C\"),2,name=\"BILINEAR_AS_DEL_DIM\")\n",
    "      \n",
    "  start_ = tf.nn.softmax(start_,axis = 1,name=\"BILINEAR_AS_SOFTMAX\")\n",
    "      \n",
    "  ################ end prediction ######################\n",
    "  end = layers.Dense(params['rnn_units'],name=\"BILINEAR_AE_SPAN\")(q_output)\n",
    "\n",
    "  hidden_end_time_axis = tf.expand_dims(end, 2, name=\"BILINEAR_AE_ADD_DIM\")\n",
    "\n",
    "  # squeeze remooves time slice we added before\n",
    "  # final shape = (batch_size,decoder_timesteps)\n",
    "  end_ = tf.squeeze(tf.matmul(c_output,hidden_end_time_axis,name=\"BILINEAR_AE_MATMUL_Q_C\"),2,name=\"BILINEAR_AE_DEL_DIM\")\n",
    "  end_ = tf.nn.softmax(end_,axis=1,name=\"BILINEAR_AE_SOFTMAX\")\n",
    "\n",
    "  prob_token_span = tf.concat((start_,end_),axis = 1,name=\"BILINEAR_AS_AE_CONCAT\")\n",
    "  \n",
    "print(\"Probab shape \",prob_token_span)\n",
    "\n",
    "\n",
    "# logits = BilinearSimilarity(UNITS)(q_cont,c_)\n",
    "# Y_prob = Prediction()(logits)\n",
    "# print(\"Logits shape \",logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibcGeRqUxzKN"
   },
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vFbuhviAyX7l",
    "outputId": "390a2ff7-8ebe-48ca-9dc9-e9f18c2d43f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probab shape  Tensor(\"START_PROBAB:0\", shape=(None, 426), dtype=float32)\n",
      "(None, 852)\n"
     ]
    }
   ],
   "source": [
    "####### Prediction ### \n",
    "token_span = 20\n",
    "with strategy.scope():\n",
    "  start_prob = tf.identity(prob_token_span[:,:params['context_max_length']],\n",
    "                          name=\"START_PROBAB\")\n",
    "  # start_prob.name = \"START_PROBAB\"\n",
    "\n",
    "  end_prob = tf.identity(prob_token_span[:,params['context_max_length']:],\n",
    "                        name=\"END_PROBAB\")\n",
    "  # end_prob.name = \"END_PROBAB\"\n",
    "  print(\"Probab shape \",start_prob)\n",
    "\n",
    "  # do the outer product\n",
    "  outer = tf.matmul(tf.expand_dims(start_prob, axis=2, name=\"PREDICT_AS_PROBAB\"),tf.expand_dims(end_prob, axis=1, name=\"PREDICT_AS_PROBAB\"),name=\"PREDICT_AS_AE_MATMUL\")\n",
    "\n",
    "  outer = tf.linalg.band_part(outer, 0, token_span,name=\"PREDICT_AS_AE_TOPTRIANGLE\")\n",
    "\n",
    "  # start_position will have shape of (batch_size,)\n",
    "  start_position = tf.reduce_max(outer, axis=2,name=\"PREDICT_AS_MAX\")\n",
    "  #end position will have shape of (batch_size,)\n",
    "  end_position = tf.reduce_max(outer, axis=1,name=\"PREDICT_AE_MAX\")\n",
    "\n",
    "  y_probab = tf.concat([start_position,end_position],axis=1,name=\"PREDICT_AS_AE\")\n",
    "\n",
    "print(y_probab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLtlPSfLxyzB"
   },
   "source": [
    "### 4.3 Custom Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlOkeMveyCWM"
   },
   "outputs": [],
   "source": [
    "def logits_loss(y_true,logits):\n",
    "    \"\"\"\n",
    "    Custom loss function which minimises log_loss.\n",
    "    Referance https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n",
    "    \"\"\"\n",
    "    \n",
    "    #y_true = tf.cast(y_true,dtype=tf.int32)\n",
    "    #logits = tf.cast(logits,dtype=tf.float32)\n",
    "    \n",
    "    # breaking the tensor into two half's to get start and end label.\n",
    "    start_label = y_true[:,:params['context_max_length']]\n",
    "    end_label = y_true[:,params['context_max_length']:]\n",
    "    \n",
    "    # breaking the logits tensor into start and end part for loss calcultion.\n",
    "    start_logit = logits[:,:params['context_max_length']]\n",
    "    end_logit = logits[:,params['context_max_length']:]\n",
    "    \n",
    "#     start_logit = tf.cast(start_logit,tf.float32)\n",
    "#     end_logit = tf.cast(end_logit,tf.float32)    \n",
    "#     print(start_label,start_logit)\n",
    "#     print(end_label,end_logit)\n",
    "   \n",
    "    \n",
    "    start_loss = tf.keras.backend.categorical_crossentropy(start_label,start_logit)\n",
    "    \n",
    "    end_loss = tf.keras.backend.categorical_crossentropy(end_label,end_logit)\n",
    "    \n",
    "    # as per paper\n",
    "    \n",
    "    loss = start_loss + end_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNiC79-tyLmq"
   },
   "source": [
    "### 4.4 Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Rv9aw2EewJbr",
    "outputId": "00fb0bcd-ce0b-4957-e2c1-ffc092229200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "QUESTION_INPUT (InputLayer)     [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QUESTION_EMBEDDING (Embedding)  (None, 40, 300)      30255300    QUESTION_INPUT[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CONTEXT_INPUT (InputLayer)      [(None, 426)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QUESTION_LSTM (Bidirectional)   (None, 256)          1140736     QUESTION_EMBEDDING[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "CONTEXT_EMBEDDING (Embedding)   (None, 426, 300)     30255300    CONTEXT_INPUT[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "BILINEAR_AS_SPAN (Dense)        (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "BILINEAR_AE_SPAN (Dense)        (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "CONTEXT_LSTM (Bidirectional)    (None, 426, 256)     1140736     CONTEXT_EMBEDDING[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BILINEAR_AS_ADD_DIM [(None, 256, 1)]     0           BILINEAR_AS_SPAN[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BILINEAR_AE_ADD_DIM [(None, 256, 1)]     0           BILINEAR_AE_SPAN[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2 (Tens [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
      "                                                                 tf_op_layer_BILINEAR_AS_ADD_DIM[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_1 (Te [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
      "                                                                 tf_op_layer_BILINEAR_AE_ADD_DIM[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BILINEAR_AS_DEL_DIM [(None, 426)]        0           tf_op_layer_BatchMatMulV2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BILINEAR_AE_DEL_DIM [(None, 426)]        0           tf_op_layer_BatchMatMulV2_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BILINEAR_AS_SOFTMAX [(None, 426)]        0           tf_op_layer_BILINEAR_AS_DEL_DIM[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BILINEAR_AE_SOFTMAX [(None, 426)]        0           tf_op_layer_BILINEAR_AE_DEL_DIM[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BILINEAR_AS_AE_CONC [(None, 852)]        0           tf_op_layer_BILINEAR_AS_SOFTMAX[0\n",
      "                                                                 tf_op_layer_BILINEAR_AE_SOFTMAX[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 426)]        0           tf_op_layer_BILINEAR_AS_AE_CONCAT\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 426)]        0           tf_op_layer_BILINEAR_AS_AE_CONCAT\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_START_PROBAB (Tenso [(None, 426)]        0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_END_PROBAB (TensorF [(None, 426)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_PREDICT_AS_PROBAB ( [(None, 426, 1)]     0           tf_op_layer_START_PROBAB[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_PREDICT_AS_PROBAB_1 [(None, 1, 426)]     0           tf_op_layer_END_PROBAB[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_2 (Te [(None, 426, 426)]   0           tf_op_layer_PREDICT_AS_PROBAB[0][\n",
      "                                                                 tf_op_layer_PREDICT_AS_PROBAB_1[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_PREDICT_AS_AE_TOPTR [(None, 426, 426)]   0           tf_op_layer_BatchMatMulV2_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_PREDICT_AS_MAX (Ten [(None, 426)]        0           tf_op_layer_PREDICT_AS_AE_TOPTRIA\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_PREDICT_AE_MAX (Ten [(None, 426)]        0           tf_op_layer_PREDICT_AS_AE_TOPTRIA\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_PREDICT_AS_AE (Tens [(None, 852)]        0           tf_op_layer_PREDICT_AS_MAX[0][0] \n",
      "                                                                 tf_op_layer_PREDICT_AE_MAX[0][0] \n",
      "==================================================================================================\n",
      "Total params: 62,923,656\n",
      "Trainable params: 2,413,056\n",
      "Non-trainable params: 60,510,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = [q_input,c_input],outputs =y_probab)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Biepd_j518BQ"
   },
   "source": [
    "### 4.5 Model Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uAqT6LWbXxWE"
   },
   "source": [
    "**Tensorboard Logs and Model compilation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kTB5bE8I2Al-",
    "outputId": "22ad60e9-cc40-46cd-ea56-767db73b9a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensprflow logs  /notebooks/tensorboard-logs/lstm-baseline0\n"
     ]
    }
   ],
   "source": [
    "# using tensorboard instance for callbacks\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = tensorboard_logpath +\"lstm-baseline0\"\n",
    "print('Tensprflow logs ',log_dir)\n",
    "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
    "with strategy.scope():\n",
    "  # model compilation\n",
    "  model.compile(optimizer=\"adamax\",loss=logits_loss,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WuaPHlU8hnp"
   },
   "source": [
    "### 4.6 Generator Function for use in Model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYIgojpb8g82"
   },
   "outputs": [],
   "source": [
    "## Reference \n",
    "def generator_function(length,batch_size = 64,data_type = 'Train'):\n",
    "    \"\"\"\n",
    "    This function is generates batches of data to avoid strain on memory.\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    flag = True\n",
    "    if data_type == 'Val':\n",
    "        flag = False\n",
    "    n = 0\n",
    "    # loop forever over datapoints.\n",
    "    while 1:\n",
    "        for i in range(length):\n",
    "            n += 1\n",
    "            if flag:\n",
    "                X1.append(train_question_sequence[i])\n",
    "                X2.append(train_context_sequence[i])                \n",
    "                y.append(y_train[i])\n",
    "            else:\n",
    "                X1.append(val_question_sequence[i])\n",
    "                X2.append(val_context_sequence[i])                \n",
    "                y.append(y_val[i])\n",
    "            if n == batch_size:\n",
    "                yield ((array(X1),array(X2)),array(y))\n",
    "                X1,X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlGf9CAe-ZW-"
   },
   "source": [
    "### 4.7 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "bO-l9AhD-fPM",
    "outputId": "300e7af6-4a1e-428a-8547-bd9c1d505ff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_length_99': 285,\n",
      " 'context_max_length': 426,\n",
      " 'context_pad_seq': 'pre',\n",
      " 'embedding_size': 300,\n",
      " 'question_length_99': 20,\n",
      " 'question_max_length': 40,\n",
      " 'question_pad_seq': 'pre',\n",
      " 'rnn_units': 256,\n",
      " 'test_shape': (26062, 16),\n",
      " 'test_span_outofrange': 0,\n",
      " 'train_shape': (78183, 16),\n",
      " 'train_span_outofrange': 0,\n",
      " 'training.batch_size': 64,\n",
      " 'training.epochs': 10,\n",
      " 'training.train_length': 78183,\n",
      " 'training.train_steps': 1221,\n",
      " 'training.val_length': 26061,\n",
      " 'training.val_steps': 814,\n",
      " 'val_shape': (26061, 16),\n",
      " 'val_span_outofrange': 0,\n",
      " 'vocab_size': 100850}\n",
      "params.jsop updated and can be found in  /storage/models/params.json\n"
     ]
    }
   ],
   "source": [
    "params['training.epochs']=10\n",
    "params['training.batch_size']=64\n",
    "params['training.train_length']=len(y_train)\n",
    "params['training.val_length']=len(y_val)\n",
    "params['training.train_steps']=params['training.train_length']//params['training.batch_size']\n",
    "params['training.val_steps']=params['training.val_length']//32\n",
    "\n",
    "pprint.pprint(params)\n",
    "\n",
    "### SAVE PARAMS\n",
    "# Writing to sample.json \n",
    "updateparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=model_path,\n",
    "                                            save_weights_only=True,\n",
    "                                            monitor='val_acc',\n",
    "                                            mode='max',\n",
    "                                            save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dfbn-RnO_EOc",
    "outputId": "38585700-1537-4494-c753-cae755275de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 start at time  2020-06-14 15:06:30.816484\n",
      " 783/1221 [==================>...........] - ETA: 10:47 - loss: 7.5192 - accuracy: 0.4710WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1221/1221 [==============================] - 2107s 2s/step - loss: 7.5227 - accuracy: 0.4701 - val_loss: 7.5104 - val_accuracy: 0.4712\n",
      "Epoch 1 start at time  2020-06-14 15:41:56.877326\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 7.4193 - accuracy: 0.4701WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1221/1221 [==============================] - 2101s 2s/step - loss: 7.4193 - accuracy: 0.4701 - val_loss: 7.5220 - val_accuracy: 0.4712\n",
      "Epoch 2 start at time  2020-06-14 16:16:59.203274\n",
      "  68/1221 [>.............................] - ETA: 31:05 - loss: 6.6991 - accuracy: 0.4800"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  for i in range(params['training.epochs']):\n",
    "      print(\"Epoch {} start at time \".format(i),datetime.now())\n",
    "      \n",
    "      train_generator = generator_function(params['training.train_length'],\n",
    "                                          params['training.batch_size'])\n",
    "      \n",
    "      val_generator = generator_function(params['training.val_length'],\n",
    "                                        32,\n",
    "                                        \"Val\")\n",
    "      model.fit(x=train_generator, epochs=1, \n",
    "                          steps_per_epoch=params['training.train_steps'],\n",
    "                          verbose=1,\n",
    "                          callbacks=[model_checkpoint_callback],\n",
    "                          validation_data=val_generator,\n",
    "                          validation_steps=params['training.val_steps'])\n",
    "    \n",
    "model.save_weights(model_path + \"context_withoutstopwords_model_epoch_10_bilstm.h5\")  \n",
    "# full model save\n",
    "model.save(model_path + \"full_context_withoutstopwords_model_epoch_10_bilstm.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRoOUajk492o"
   },
   "source": [
    "### 4.6 Serialize and Persist Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnnjf-hX2FD7"
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path + \"context_withoutstopwords_model_epoch_25_bilstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELL6OBOaFud5"
   },
   "outputs": [],
   "source": [
    "# full model save\n",
    "model.save(model_path + \"full_context_withoutstopwords_model_epoch_25_bilstm.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jl0YsArymDdF"
   },
   "source": [
    "### 4.7 Load existing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBwGJN07mEGY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e4680651687d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodelname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'context_withoutstopwords_model_epoch_10_bilstm.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# modelname = 'model_epoch_24.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodelname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "modelname = 'context_withoutstopwords_model_epoch_10_bilstm.h5'\n",
    "# modelname = 'model_epoch_24.h5'\n",
    "model.load_weights(model_path + modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=607263109331-gaed96n8hjeqf7er45au9ifj0834geog.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/0wHUWm3KZLPUks0u4Gdq4XIUBQSVZGiDNCmBGIoAKYvvwf5DSH2z7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "gauth = GoogleAuth()\n",
    "gauth.CommandLineAuth()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google-api-python-client\n",
      "Version: 1.9.3\n",
      "Summary: Google API Client Library for Python\n",
      "Home-page: https://github.com/googleapis/google-api-python-client/\n",
      "Author: Google LLC\n",
      "Author-email: googleapis-packages@google.com\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python3.6/dist-packages\n",
      "Requires: google-api-core, httplib2, six, google-auth-httplib2, uritemplate, google-auth\n",
      "Required-by: PyDrive\n",
      "Collecting httplib2==0.15.0\n",
      "  Using cached httplib2-0.15.0-py3-none-any.whl (94 kB)\n",
      "Installing collected packages: httplib2\n",
      "  Attempting uninstall: httplib2\n",
      "    Found existing installation: httplib2 0.18.1\n",
      "    Uninstalling httplib2-0.18.1:\n",
      "      Successfully uninstalled httplib2-0.18.1\n",
      "Successfully installed httplib2-0.15.0\n",
      "Collecting google-api-python-client==1.6\n",
      "  Downloading google_api_python_client-1.6.0-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.6) (1.15.0)\n",
      "Requirement already satisfied: oauth2client<5.0.0dev,>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.6) (4.1.3)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.6) (3.0.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.6) (0.15.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (4.6)\n",
      "Installing collected packages: google-api-python-client\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 1.9.3\n",
      "    Uninstalling google-api-python-client-1.9.3:\n",
      "      Successfully uninstalled google-api-python-client-1.9.3\n",
      "Successfully installed google-api-python-client-1.6.0\n"
     ]
    }
   ],
   "source": [
    "# set httplib2 to 1.5.0\n",
    "# !pip install -U httplib2==0.15.0\n",
    "!pip show google-api-python-client\n",
    "# !pip install -U httplib2 \n",
    "\n",
    "!pip install httplib2==0.15.0\n",
    "\n",
    "!pip install google-api-python-client==1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RedirectMissingLocation",
     "evalue": "Redirected but the response is missing a Location: header.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRedirectMissingLocation\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d80603d3b59a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mupload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'context_withoutstopwords_model_epoch_10_bilstm.h5'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mupload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetContentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'context_withoutstopwords_model_epoch_10_bilstm.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mupload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/files.py\u001b[0m in \u001b[0;36mUpload\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FilesPatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FilesInsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mTrash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGet_Http_Object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecoratee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_decorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/files.py\u001b[0m in \u001b[0;36m_FilesInsert\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'media_body'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_BuildMediaBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       metadata = self.auth.service.files().insert(**param).execute(\n\u001b[0;32m--> 369\u001b[0;31m         http=self.http)\n\u001b[0m\u001b[1;32m    370\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mApiRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m         \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m           \u001b[0mprint\u001b[0m \u001b[0;34m\"Upload %d%% complete.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mnext_chunk\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m   \u001b[0mExample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogleapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchHttpRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlist_animals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/oauth2client/transport.py\u001b[0m in \u001b[0;36mnew_request\u001b[0;34m(uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m    173\u001b[0m         resp, content = request(orig_request_method, uri, method, body,\n\u001b[1;32m    174\u001b[0m                                 \u001b[0mclean_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                 redirections, connection_type)\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# A stored token may expire between the time it is retrieved and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/oauth2client/transport.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(http, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m    280\u001b[0m     return http_callable(uri, method=method, body=body, headers=headers,\n\u001b[1;32m    281\u001b[0m                          \u001b[0mredirections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                          connection_type=connection_type)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m   1992\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"Request Timeout\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m                     response = Response(\n\u001b[0;32m-> 1994\u001b[0;31m                         {\n\u001b[0m\u001b[1;32m   1995\u001b[0m                             \u001b[0;34m\"content-type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"text/plain\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m                             \u001b[0;34m\"status\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"408\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001b[0m\n\u001b[1;32m   1688\u001b[0m                             )\n\u001b[1;32m   1689\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m301\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m                         \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"-x-permanent-redirect-url\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"location\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;34m\"content-location\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m                             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content-location\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabsolute_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRedirectMissingLocation\u001b[0m: Redirected but the response is missing a Location: header."
     ]
    }
   ],
   "source": [
    "upload = drive.CreateFile({'title': 'context_withoutstopwords_model_epoch_10_bilstm.h5'})\n",
    "upload.SetContentFile(model_path + 'context_withoutstopwords_model_epoch_10_bilstm.h5')\n",
    "upload.Upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tVohk7bORpH"
   },
   "source": [
    "### 4.8 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7pxr5yHOV6m"
   },
   "source": [
    "#### 4.8.1 Eval on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "8VEUBrVHN6c_",
    "outputId": "466d88e2-e2f2-48ad-8d29-f0ef221c4f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RebatchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AutoShardDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MultiDeviceIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_distributed_function_113429 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict([test_question_sequence,test_context_sequence])\n",
    "# print y_prediction[0] should return probabilty of of each index been a start and end token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CnVyS4ONOiNO"
   },
   "outputs": [],
   "source": [
    "# y_test was a list changing to numpy array\n",
    "y_test_fixed = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_XCfB-gOpuh"
   },
   "outputs": [],
   "source": [
    "# argmax is used to get the index where the max value in a list appears, and hence \n",
    "# for every index i, we can get the place of start and end token of the max probab\n",
    "start_pred = []\n",
    "end_pred = []\n",
    "for i in range(26062):\n",
    "    start_pred.append(np.argmax(y_prediction[i,:params['context_max_length']]))\n",
    "    end_pred.append(np.argmax(y_prediction[i,params['context_max_length']:]))\n",
    "    \n",
    "# compute for y_test though in this case it the max of 0 and 1 for \n",
    "# the frist half od array size for start, and rest for end\n",
    "start = []\n",
    "end = []\n",
    "for i in range(26062):\n",
    "    start.append(np.argmax(y_test_fixed[i,:params['context_max_length']]))\n",
    "    end.append(np.argmax(y_test_fixed[i,params['context_max_length']:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "138CmIXnR7LS",
    "outputId": "b4fce02a-ef60-494d-eef9-959c90cfbb46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[425, 122, 46, 9, 425, 425, 21, 12, 425, 65, 54, 69, 425, 37, 17, 425, 2, 425, 21, 5]\n",
      "[425, 125, 52, 9, 425, 425, 21, 12, 425, 66, 55, 70, 425, 37, 17, 425, 2, 425, 21, 5]\n"
     ]
    }
   ],
   "source": [
    "print(start[100:120])\n",
    "print(end[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UPx7_OKMR8fX"
   },
   "outputs": [],
   "source": [
    "y_predicted_new = np.zeros((26062,params['context_max_length']))\n",
    "for i in range(26062):\n",
    "    y_predicted_new[i,start_pred[i]:end_pred[i]+1] = 1\n",
    "    \n",
    "y_test_new = np.zeros((26062,params['context_max_length']))\n",
    "for i in range(26062):\n",
    "    y_test_new[i,start[i]:end[i]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "0tD4wUQ09SRb",
    "outputId": "cbaa5f31-16f0-49ca-f8b6-6d0e92af2ec7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9c2aeaea85e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'testindex' is not defined"
     ]
    }
   ],
   "source": [
    "len(y_test_new[testindex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v78fhG3Tdhx5"
   },
   "source": [
    "#### 4.8.2 Create a common function to predict and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6KDfM25dhZr"
   },
   "outputs": [],
   "source": [
    "def predit_test(context, question):\n",
    "  # get sequence for context and question\n",
    "  c_ = preprocess_text(context)\n",
    "  q_ = preprocess_text(question,stopword_removal=False)\n",
    "  c,q = generate_question_context_sequence(c_, q_)  \n",
    "  y_ = model.predict([q,c])    \n",
    "  # # for i in range(26062):\n",
    "  s = np.argmax(y_[0,:params['context_max_length']])\n",
    "  e = np.argmax(y_[0,params['context_max_length']:])\n",
    "  answer = span_to_answer((s,e),c_[0])\n",
    "  \n",
    "  # print(c.shape,q.shape,y_.shape,s,e,answer)  \n",
    "  # print(s, e)\n",
    "  return c_,q_,[s,e],y_,answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNqWwnNX5EAF"
   },
   "source": [
    "##### 4.8.2.1 TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "aqKrcmCae0z8",
    "outputId": "6e087e47-0a88-4f4c-a505-3273e90ea17e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ori c = \n",
      "('In Mark, Jesus is crucified along with two rebels, and the day goes dark for '\n",
      " 'three hours. Jesus calls out to God, then gives a shout and dies. The '\n",
      " 'curtain of the Temple is torn in two. Matthew follows Mark, adding an '\n",
      " 'earthquake and the resurrection of saints. Luke also follows Mark, though he '\n",
      " 'describes the rebels as common criminals, one of whom defends Jesus, who in '\n",
      " 'turn promises that he (Jesus) and the criminal will be together in paradise. '\n",
      " 'Luke portrays Jesus as impassive in the face of his crucifixion. John '\n",
      " 'includes several of the same elements as those found in Mark, though they '\n",
      " 'are treated differently.')\n",
      "ori c c = \n",
      "('mark jesus crucified along two rebels day goes dark three hours jesus calls '\n",
      " 'god gives shout dies curtain temple torn two matthew follows mark adding '\n",
      " 'earthquake resurrection saints luke also follows mark though describes '\n",
      " 'rebels common criminals one defends jesus turn promises jesus criminal '\n",
      " 'together paradise luke portrays jesus impassive face crucifixion john '\n",
      " 'includes several elements found mark though treated differently')\n",
      "ori q =  what does luke say one of the rebels does\n",
      "new c\n",
      "('mahayana buddha tends viewed merely human earthly projection beginningless '\n",
      " 'endless omnipresent see dharmakaya beyond range reach thought moreover '\n",
      " 'certain mahayana sutras buddha dharma sangha viewed essentially one three '\n",
      " 'seen eternal buddha')\n",
      "new q ['in what sutras are the buddha dharma and sangha viewed as one']\n",
      "predicted answer \n"
     ]
    }
   ],
   "source": [
    "c='In the Mahayana, the Buddha tends not to be viewed as merely human, but as the earthly projection of a beginningless and endless, omnipresent being (see Dharmakaya) beyond the range and reach of thought. Moreover, in certain Mahayana sutras, the Buddha, Dharma and Sangha are viewed essentially as One: all three are seen as the eternal Buddha himself.'\n",
    "q='in what sutras are the buddha dharma and sangha viewed as one'\n",
    "\n",
    "# c_,q_,span,y_,answer = predit_test(test['context'].iloc[39],test['question'].iloc[39])\n",
    "c_,q_,span,y_,answer = predit_test([c],[q])\n",
    "print('ori c = ')\n",
    "pprint.pprint(test['context'].iloc[39])\n",
    "print('ori c c = ')\n",
    "pprint.pprint(test['clean_context'].iloc[39])\n",
    "print('ori q = ',test['clean_question'].iloc[39])\n",
    "print('new c')\n",
    "pprint.pprint(c_[0])\n",
    "print('new q',q_)\n",
    "\n",
    "print('predicted answer' ,answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PEKF0GU5SZg"
   },
   "source": [
    "##### 4.8.2.2 TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YnSg9CSCxy2k",
    "outputId": "0de5b7bc-94b3-4a03-f2c2-6a0ca974759e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted answer \n"
     ]
    }
   ],
   "source": [
    "c = 'Mary went to the bathroom. John is in the playground.John moved to the hallway. John picked up the football.Mary travelled to the office'\n",
    "q = 'Where is john?'\n",
    "c_,q_,span,y_,answer = predit_test([c],[q])\n",
    "print('predicted answer' ,answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUF1OjiM5Uh5"
   },
   "source": [
    "##### 4.8.2.3 TEST 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jSBOvRP90Rs9",
    "outputId": "c68ae798-cbd2-4599-d4ae-14eacbfd2e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted answer \n"
     ]
    }
   ],
   "source": [
    "c='The Union health ministry said that so far, 95,527 COVID-19 patients have recovered in the country.The recovery rate is now 48.07 percent, Lav Agrawal, Joint Secretary, Health Ministry claimed. We have asked all states to analyse the trajectory of the cases in their respective states. If a state thinks that it needs to set up temporary COVID-19 care centres then it must do so, he added.'\n",
    "q='what is the recovery rate'\n",
    "c_,q_,span,y_,answer = predit_test([c],[q])\n",
    "print('predicted answer' ,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wMaE2PJw1TnO",
    "outputId": "4a4aa151-3f49-4659-80f1-9aac736c3710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted answer \n"
     ]
    }
   ],
   "source": [
    "c='The Union health ministry said that so far, 95,527 COVID-19 patients have recovered in the country.The recovery rate is now 48.07 percent, Lav Agrawal, Joint Secretary, Health Ministry claimed. We have asked all states to analyse the trajectory of the cases in their respective states. If a state thinks that it needs to set up temporary COVID-19 care centres then it must do so, he added.'\n",
    "q='wh is the name'\n",
    "c_,q_,span,y_,answer = predit_test([c],[q])\n",
    "print('predicted answer' ,answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHxti5JDP4UD"
   },
   "source": [
    "#### 4.8.3 See true vs predict for one samples in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "y9qs-no68n52",
    "outputId": "ef349c56-e34f-4a63-8116-10988ff846e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ori Cont = \n",
      "('In the following months, NATO took a wide range of measures to respond to '\n",
      " 'the threat of terrorism. On 22 November 2002, the member states of the '\n",
      " 'Euro-Atlantic Partnership Council (EAPC) decided on a Partnership Action '\n",
      " 'Plan against Terrorism, which explicitly states, \"EAPC States are committed '\n",
      " 'to the protection and promotion of fundamental freedoms and human rights, as '\n",
      " 'well as the rule of law, in combating terrorism.\" NATO started naval '\n",
      " 'operations in the Mediterranean Sea designed to prevent the movement of '\n",
      " 'terrorists or weapons of mass destruction as well as to enhance the security '\n",
      " 'of shipping in general called Operation Active Endeavour.')\n",
      "CLean Cont = \n",
      "('following months nato took wide range measures respond threat terrorism 22 '\n",
      " 'november 2002 member states euroatlantic partnership council eapc decided '\n",
      " 'partnership action plan terrorism explicitly states eapc states committed '\n",
      " 'protection promotion fundamental freedoms human rights well rule law '\n",
      " 'combating terrorism nato started naval operations mediterranean sea designed '\n",
      " 'prevent movement terrorists weapons mass destruction well enhance security '\n",
      " 'shipping general called operation active endeavour')\n",
      "Question =  What were NATO's efforts to respond to terrorism called?\n",
      "Clean Question =  what were natos efforts to respond to terrorism called\n",
      "Answer =  nan\n",
      "Clean Answer =  IMPOSSIBLE\n",
      "AS,AE =  (-1, -1)\n",
      "pAS,pAE =  (425, 425)\n",
      "Predict answer = \n",
      "test data encoded  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "predict data  encoded  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "testindex = 54\n",
    "print(\"Ori Cont = \")\n",
    "pprint.pprint(test['context'].iloc[testindex])\n",
    "print(\"CLean Cont = \")\n",
    "pprint.pprint(test['clean_context'].iloc[testindex])\n",
    "print('Question = ',test['question'].iloc[testindex])\n",
    "print('Clean Question = ',test['clean_question'].iloc[testindex])\n",
    "print('Answer = ',test['answer'].iloc[testindex])\n",
    "print('Clean Answer = ',test['clean_answer'].iloc[testindex])\n",
    "print('AS,AE = ',test['answer_word_span'].iloc[testindex])\n",
    "print('pAS,pAE = ',(start_pred[testindex],end_pred[testindex]))\n",
    "print(\"Predict answer =\",span_to_answer([start_pred[testindex],end_pred[testindex]],test['clean_context'].iloc[testindex]))\n",
    "# print(\"encoded len\", len(y_train[testindex]))\n",
    "# print(\"encoded \", len(y_test[testindex]))\n",
    "print(\"test data encoded \",y_test_new[testindex])\n",
    "print(\"predict data  encoded \",y_predicted_new[testindex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYHNErgrQILC"
   },
   "source": [
    "#### 4.8.4 Accuracy Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "FmwzLM3WSriL",
    "outputId": "444e9119-5293-4c4a-eedd-0f50482f59f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro f1-score on test data is  0.11369224094294597\n",
      "Macro f1-score on test data is  0.03994890959380932\n",
      "Accuracy on test data is  0.0682603023559205\n",
      "params.jsop updated and can be found in  /content/drive/My Drive/AIML-MRC-Capstone/models/params.json\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score\n",
    "params['prediction.accuracy.score'] = accuracy_score(y_test_new,y_predicted_new)\n",
    "params['prediction.macrof1.score'] = f1_score(y_test_new,y_predicted_new,average=\"macro\")\n",
    "params['prediction.microf1.score'] = f1_score(y_test_new,y_predicted_new,average=\"micro\")\n",
    "\n",
    "print(\"Micro f1-score on test data is \",params['prediction.microf1.score'])\n",
    "print(\"Macro f1-score on test data is \",params['prediction.macrof1.score'])\n",
    "print(\"Accuracy on test data is \",params['prediction.accuracy.score'])\n",
    "\n",
    "# update params\n",
    "updateparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "RY02U6dQSCMU",
    "outputId": "b303a5cf-397a-4bbf-f6a3-c0141718c752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_length_99': 285,\n",
      " 'context_max_length': 426,\n",
      " 'context_pad_seq': 'post',\n",
      " 'embedding_size': 300,\n",
      " 'prediction.accuracy.score': 0.0682603023559205,\n",
      " 'prediction.macrof1.score': 0.03994890959380932,\n",
      " 'prediction.microf1.score': 0.11369224094294597,\n",
      " 'question_length_99': 20,\n",
      " 'question_max_length': 40,\n",
      " 'question_pad_seq': 'post',\n",
      " 'rnn_units': 256,\n",
      " 'test_shape': (26062, 16),\n",
      " 'test_span_outofrange': 0,\n",
      " 'train_shape': (78183, 16),\n",
      " 'train_span_outofrange': 0,\n",
      " 'training.batch_size': 64,\n",
      " 'training.epochs': 25,\n",
      " 'training.train_length': 78183,\n",
      " 'training.train_steps': 1221,\n",
      " 'training.val_length': 26061,\n",
      " 'training.val_steps': 814,\n",
      " 'val_shape': (26061, 16),\n",
      " 'val_span_outofrange': 0,\n",
      " 'vocab_size': 100850}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rI40rd0bQZgn"
   },
   "source": [
    "#### 4.8.5 Store the result to build more meterics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5T9BSNgS-uG"
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "summary = PrettyTable()\n",
    "summary.title = \"Test vs Prediction\"\n",
    "summary.field_names = [\"ID\",\n",
    "                       \"Clean Question\",\n",
    "                       \"Clean Context\",\n",
    "                       \"True Answer\",\n",
    "                       \"True AS and AE\",\n",
    "                       \"Predict Answer\",\n",
    "                       \"Predict AS and AE\"]\n",
    "result_df = pd.DataFrame(columns=summary.field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6TdUqImSC1_I",
    "outputId": "0d434946-0b08-4dbc-eefa-2f245f9ad7a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26062/26062 [03:14<00:00, 134.29it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(26062)):  \n",
    "  values = [test['id'].iloc[i], \n",
    "            test['clean_question'].iloc[i], \n",
    "            test['clean_context'].iloc[i], \n",
    "            test['clean_answer'].iloc[i], \n",
    "            test['answer_word_span'].iloc[i],\n",
    "            span_to_answer([start_pred[i],end_pred[i]],test['clean_context'].iloc[i]),\n",
    "            (start_pred[i],end_pred[i])]\n",
    "  zipped = zip(summary.field_names, values)\n",
    "  a_dictionary = dict(zipped)\n",
    "  result_df = result_df.append(a_dictionary,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "TySMZHmbJB1r",
    "outputId": "507594a5-738a-48c6-d281-8fd8297427f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Clean Question</th>\n",
       "      <th>Clean Context</th>\n",
       "      <th>True Answer</th>\n",
       "      <th>True AS and AE</th>\n",
       "      <th>Predict Answer</th>\n",
       "      <th>Predict AS and AE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572710d0f1498d1400e8f2ed</td>\n",
       "      <td>who is originally claimed to have given birth ...</td>\n",
       "      <td>nutritionism view excessive reliance food scie...</td>\n",
       "      <td>gyorgy scrinis</td>\n",
       "      <td>(15, 16)</td>\n",
       "      <td>rely</td>\n",
       "      <td>(25, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56f8cc7b9e9bad19000a0520</td>\n",
       "      <td>the neocerebellum supports what other part of ...</td>\n",
       "      <td>elaboration cerebral cortex carries changes br...</td>\n",
       "      <td>cerebral cortex</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td></td>\n",
       "      <td>(425, 425)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570d9e64df2f5219002ed063</td>\n",
       "      <td>what is the hmmwv also known as</td>\n",
       "      <td>armys common vehicle high mobility multipurpos...</td>\n",
       "      <td>humvee</td>\n",
       "      <td>(11, 11)</td>\n",
       "      <td>armys</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56d1070517492d1400aab77a</td>\n",
       "      <td>what was a notable 20th century gang in new york</td>\n",
       "      <td>organized crime long associated new york city ...</td>\n",
       "      <td>the black spades</td>\n",
       "      <td>(-1, -1)</td>\n",
       "      <td></td>\n",
       "      <td>(425, 425)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56fb7df48ddada1400cd6481</td>\n",
       "      <td>along with len portugal aragon and castile wha...</td>\n",
       "      <td>iberia christian states confined northwestern ...</td>\n",
       "      <td>navarre</td>\n",
       "      <td>(26, 26)</td>\n",
       "      <td>iberia</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID  ... Predict AS and AE\n",
       "0  572710d0f1498d1400e8f2ed  ...          (25, 25)\n",
       "1  56f8cc7b9e9bad19000a0520  ...        (425, 425)\n",
       "2  570d9e64df2f5219002ed063  ...            (0, 0)\n",
       "3  56d1070517492d1400aab77a  ...        (425, 425)\n",
       "4  56fb7df48ddada1400cd6481  ...            (0, 0)\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.to_csv(model_path + \"results.csv\")  \n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCKNvmOaQtx_"
   },
   "source": [
    "## 5 More Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2emUc95Q53a"
   },
   "source": [
    "**Read the result dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "geCm-V3SQ0v7",
    "outputId": "1e4348c8-88d9-46e4-9800-f229fd1fcaab"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-447b545bbd02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"results.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'read_csv'"
     ]
    }
   ],
   "source": [
    "result_df = result_df.read_csv(model_path + \"results.csv\")  \n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mz6uetkTRLrz"
   },
   "source": [
    "### 5.1 EM (Exact Match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "5uTD7fZZUhyw",
    "outputId": "35252419-08c0-4602-c209-892e0180d994"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Clean Question</th>\n",
       "      <th>Clean Context</th>\n",
       "      <th>True Answer</th>\n",
       "      <th>True AS and AE</th>\n",
       "      <th>Predict Answer</th>\n",
       "      <th>Predict AS and AE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, Clean Question, Clean Context, True Answer, True AS and AE, Predict Answer, Predict AS and AE]\n",
       "Index: []"
      ]
     },
     "execution_count": 103,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[result_df['Predict Answer'] == result_df['True Answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rm4YmnfQNqQC"
   },
   "outputs": [],
   "source": [
    "ematch = result_df[result_df['Predict Answer'] == result_df['True Answer']].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BF44727PN1dq",
    "outputId": "4612d5f0-2b6a-4cea-d584-ed9cfda9871b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params.jsop updated and can be found in  /content/drive/My Drive/AIML-MRC-Capstone/models/params.json\n"
     ]
    }
   ],
   "source": [
    "params['prediction.em.score'] = ematch / params['test_shape'][0]\n",
    "updateparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "ZbO5yYzRTTyu",
    "outputId": "89f7b537-b591-4575-e5c4-4efb7b9b1183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_length_99': 285,\n",
      " 'context_max_length': 426,\n",
      " 'context_pad_seq': 'pre',\n",
      " 'embedding_size': 100,\n",
      " 'prediction.accuracy.score': 0.3761798787506715,\n",
      " 'prediction.em.score': 0.00690660732100376,\n",
      " 'prediction.macrof1.score': 0.00583615881279302,\n",
      " 'prediction.microf1.score': 0.2751746082042751,\n",
      " 'question_length_99': 20,\n",
      " 'question_max_length': 40,\n",
      " 'question_pad_seq': 'pre',\n",
      " 'rnn_units': 256,\n",
      " 'test_shape': (26062, 16),\n",
      " 'test_span_outofrange': 0,\n",
      " 'tokenizer_num_words': 80000,\n",
      " 'train_shape': (78183, 16),\n",
      " 'train_span_outofrange': 0,\n",
      " 'training.batch_size': 64,\n",
      " 'training.epochs': 25,\n",
      " 'training.train_length': 78183,\n",
      " 'training.train_steps': 1221,\n",
      " 'training.val_length': 26061,\n",
      " 'training.val_steps': 814,\n",
      " 'val_shape': (26061, 16),\n",
      " 'val_span_outofrange': 0,\n",
      " 'vocab_size': 100850}\n"
     ]
    }
   ],
   "source": [
    "showparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlR2BYOT66Lk"
   },
   "source": [
    "# **<font color=\"GREEN\">END OF THE NOTEBOOK </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6u50QkJn6-om"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mrc_biLSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
